{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluation = True\n",
    "evaluation_verbose = False\n",
    "\n",
    "OUTPUT_BUCKET_FOLDER = \"gs://<GCS_BUCKET_NAME>/outbrain-click-prediction/output/\"\n",
    "DATA_BUCKET_FOLDER = \"gs://<GCS_BUCKET_NAME>/outbrain-click-prediction/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.linalg import Vectors, SparseVector, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def hashstr(s, nr_bins):\n",
    "    return int(hashlib.md5(s.encode('utf8')).hexdigest(), 16)%(nr_bins-1)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_time_to_unix_epoch(date_time):\n",
    "    return int(time.mktime(date_time.timetuple()))\n",
    "\n",
    "def date_time_to_unix_epoch_treated(dt):\n",
    "    if dt != None:\n",
    "        try:\n",
    "            epoch = date_time_to_unix_epoch(dt)\n",
    "            return epoch\n",
    "        except Exception as e:\n",
    "            print(\"Error processing dt={}\".format(dt), e)\n",
    "            return 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timestamp_null_to_zero_int_udf = F.udf(lambda x: date_time_to_unix_epoch_treated(x), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INT_DEFAULT_NULL_VALUE = -1\n",
    "int_null_to_minus_one_udf = F.udf(lambda x: x if x != None else INT_DEFAULT_NULL_VALUE, IntegerType())\n",
    "int_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(IntegerType()))\n",
    "float_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(FloatType()))\n",
    "str_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def truncate_day_from_timestamp(ts):\n",
    "    return int(ts / 1000 / 60 / 60 / 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truncate_day_from_timestamp_udf = F.udf(lambda ts: truncate_day_from_timestamp(ts), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_country_udf = F.udf(lambda geo: geo.strip()[:2] if geo != None else '', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_country_state_udf = F.udf(lambda geo: geo.strip()[:5] if geo != None else '', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_len_udf = F.udf(lambda x: len(x) if x != None else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_odd_timestamp(timestamp_ms_relative):\n",
    "    TIMESTAMP_DELTA=1465876799998\n",
    "    return datetime.datetime.fromtimestamp((int(timestamp_ms_relative)+TIMESTAMP_DELTA)//1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading UTC/BST for each country and US / CA states (local time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "country_utc_dst_df = pd.read_csv('country_codes_utc_dst_tz_delta.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries_utc_dst_dict = dict(zip(country_utc_dst_df['country_code'].tolist(), country_utc_dst_df['utc_dst_time_offset_cleaned'].tolist()))\n",
    "countries_utc_dst_broad = sc.broadcast(countries_utc_dst_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_states_utc_dst_df = pd.read_csv('us_states_abbrev_bst.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_states_utc_dst_dict = dict(zip(us_states_utc_dst_df['state_abb'].tolist(), us_states_utc_dst_df['utc_dst_time_offset_cleaned'].tolist()))\n",
    "us_states_utc_dst_broad = sc.broadcast(us_states_utc_dst_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ca_states_utc_dst_df = pd.read_csv('ca_states_abbrev_bst.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ca_countries_utc_dst_dict = dict(zip(ca_states_utc_dst_df['state_abb'].tolist(), ca_states_utc_dst_df['utc_dst_time_offset_cleaned'].tolist()))\n",
    "ca_countries_utc_dst_broad = sc.broadcast(ca_countries_utc_dst_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading competition csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"uuid_event\", StringType(), True),                    \n",
    "                    StructField(\"document_id_event\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_event\", IntegerType(), True),\n",
    "                    StructField(\"platform_event\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_event\", StringType(), True)]\n",
    "                    )\n",
    "\n",
    "events_df = spark.read.schema(events_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER + \"events.csv\") \\\n",
    "                .withColumn('dummyEvents', F.lit(1)) \\\n",
    "                .withColumn('day_event', truncate_day_from_timestamp_udf('timestamp_event')) \\\n",
    "                .withColumn('event_country', extract_country_udf('geo_location_event')) \\\n",
    "                .withColumn('event_country_state', extract_country_state_udf('geo_location_event')) \\\n",
    "                .alias('events')               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page_views_schema = StructType(\n",
    "                    [StructField(\"uuid_pv\", StringType(), True),\n",
    "                    StructField(\"document_id_pv\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_pv\", IntegerType(), True),\n",
    "                    StructField(\"platform_pv\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_pv\", StringType(), True),\n",
    "                    StructField(\"traffic_source_pv\", IntegerType(), True)]\n",
    "                    )\n",
    "page_views_df = spark.read.schema(page_views_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"page_views.csv\") \\\n",
    "                .withColumn('day_pv', truncate_day_from_timestamp_udf('timestamp_pv')) \\\n",
    "                .alias('page_views')        \n",
    "            \n",
    "page_views_df.createOrReplaceTempView('page_views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page_views_users_df  = spark.sql('''\n",
    "                    SELECT uuid_pv, document_id_pv, max(timestamp_pv) as max_timestamp_pv, 1 as dummyPageView\n",
    "                    FROM page_views p \n",
    "                    GROUP BY uuid_pv, document_id_pv\n",
    "                    ''').alias('page_views_users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "promoted_content_schema = StructType(\n",
    "                    [StructField(\"ad_id\", IntegerType(), True),\n",
    "                    StructField(\"document_id_promo\", IntegerType(), True),                    \n",
    "                    StructField(\"campaign_id\", IntegerType(), True),\n",
    "                    StructField(\"advertiser_id\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "promoted_content_df = spark.read.schema(promoted_content_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"promoted_content.csv\") \\\n",
    "                .withColumn('dummyPromotedContent', F.lit(1)).alias('promoted_content').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents_meta_schema = StructType(\n",
    "                    [StructField(\"document_id_doc\", IntegerType(), True),\n",
    "                    StructField(\"source_id\", IntegerType(), True),                    \n",
    "                    StructField(\"publisher_id\", IntegerType(), True),\n",
    "                    StructField(\"publish_time\", TimestampType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_meta_df = spark.read.schema(documents_meta_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_meta.csv\") \\\n",
    "                .withColumn('dummyDocumentsMeta', F.lit(1)).alias('documents_meta').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Joining with Page Views to get traffic_source_pv\n",
    "events_joined_df = events_df.join(documents_meta_df \\\n",
    "                                  .withColumnRenamed('source_id', 'source_id_doc_event') \\\n",
    "                                  .withColumnRenamed('publisher_id', 'publisher_doc_event') \\\n",
    "                                  .withColumnRenamed('publish_time', 'publish_time_doc_event')\n",
    "                                  , on=F.col(\"document_id_event\") == F.col(\"document_id_doc\"), how='left') \\\n",
    "                            .join(page_views_df, \n",
    "                                           on=[F.col('uuid_event') == F.col('uuid_pv'),\n",
    "                                               F.col('document_id_event') == F.col('document_id_pv'),\n",
    "                                               F.col('platform_event') == F.col('platform_pv'),\n",
    "                                               F.col('geo_location_event') == F.col('geo_location_pv'),\n",
    "                                               F.col('day_event') == F.col('day_pv')],\n",
    "                                           how='left') \\\n",
    "                                    .alias('events').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents_categories_schema = StructType(\n",
    "                    [StructField(\"document_id_cat\", IntegerType(), True),\n",
    "                    StructField(\"category_id\", IntegerType(), True),                    \n",
    "                    StructField(\"confidence_level_cat\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_categories_df = spark.read.schema(documents_categories_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_categories.csv\") \\\n",
    "                .alias('documents_categories').cache()\n",
    "    \n",
    "documents_categories_grouped_df = documents_categories_df.groupBy('document_id_cat') \\\n",
    "                                            .agg(F.collect_list('category_id').alias('category_id_list'),\n",
    "                                                 F.collect_list('confidence_level_cat').alias('confidence_level_cat_list')) \\\n",
    "                                            .withColumn('dummyDocumentsCategory', F.lit(1)) \\\n",
    "                                            .alias('documents_categories_grouped')                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents_topics_schema = StructType(\n",
    "                    [StructField(\"document_id_top\", IntegerType(), True),\n",
    "                    StructField(\"topic_id\", IntegerType(), True),                    \n",
    "                    StructField(\"confidence_level_top\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_topics_df = spark.read.schema(documents_topics_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_topics.csv\")  \\\n",
    "                .alias('documents_topics').cache()\n",
    "    \n",
    "documents_topics_grouped_df = documents_topics_df.groupBy('document_id_top') \\\n",
    "                                            .agg(F.collect_list('topic_id').alias('topic_id_list'),\n",
    "                                                 F.collect_list('confidence_level_top').alias('confidence_level_top_list')) \\\n",
    "                                            .withColumn('dummyDocumentsTopics', F.lit(1)) \\\n",
    "                                            .alias('documents_topics_grouped')                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents_entities_schema = StructType(\n",
    "                    [StructField(\"document_id_ent\", IntegerType(), True),\n",
    "                    StructField(\"entity_id\", StringType(), True),                    \n",
    "                    StructField(\"confidence_level_ent\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_entities_df = spark.read.schema(documents_entities_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_entities.csv\")  \\\n",
    "                .alias('documents_entities').cache()\n",
    "    \n",
    "documents_entities_grouped_df = documents_entities_df.groupBy('document_id_ent') \\\n",
    "                                            .agg(F.collect_list('entity_id').alias('entity_id_list'),\n",
    "                                                 F.collect_list('confidence_level_ent').alias('confidence_level_ent_list')) \\\n",
    "                                            .withColumn('dummyDocumentsEntities', F.lit(1)) \\\n",
    "                                            .alias('documents_entities_grouped')                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clicks_train_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"ad_id\", IntegerType(), True),                    \n",
    "                    StructField(\"clicked\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "clicks_train_df = spark.read.schema(clicks_train_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"clicks_train.csv\") \\\n",
    "                .withColumn('dummyClicksTrain', F.lit(1)).alias('clicks_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clicks_train_joined_df = clicks_train_df \\\n",
    "                         .join(promoted_content_df, on='ad_id', how='left') \\\n",
    "                         .join(documents_meta_df, on=F.col(\"promoted_content.document_id_promo\") == F.col(\"documents_meta.document_id_doc\"), how='left') \\\n",
    "                         .join(events_joined_df, on='display_id', how='left')                         \n",
    "clicks_train_joined_df.createOrReplaceTempView('clicks_train_joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    table_name = 'user_profiles_eval'\n",
    "else:\n",
    "    table_name = 'user_profiles'\n",
    "\n",
    "user_profiles_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER+table_name) \\\n",
    "                    .withColumn('dummyUserProfiles', F.lit(1)).alias('user_profiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting Train/validation set | Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_df.count() = 87141731\n"
     ]
    }
   ],
   "source": [
    "if evaluation:       \n",
    "    validation_set_exported_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER+\"validation_set.parquet\") \\\n",
    "                    .alias('validation_set') \n",
    "            \n",
    "    validation_set_exported_df.select('display_id').distinct().createOrReplaceTempView(\"validation_display_ids\")\n",
    "    \n",
    "    \n",
    "    validation_set_df = spark.sql('''SELECT * FROM clicks_train_joined t \n",
    "             WHERE EXISTS (SELECT display_id FROM validation_display_ids \n",
    "                           WHERE display_id = t.display_id)''').alias('clicks') \\\n",
    "                         .join(documents_categories_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_categories_grouped.document_id_cat\"), how='left') \\\n",
    "                         .join(documents_topics_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_topics_grouped.document_id_top\"), how='left') \\\n",
    "                         .join(documents_entities_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_entities_grouped.document_id_ent\"), how='left') \\\n",
    "                         .join(documents_categories_grouped_df \\\n",
    "                                   .withColumnRenamed('category_id_list', 'doc_event_category_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_cat_list', 'doc_event_confidence_level_cat_list') \\\n",
    "                                   .alias('documents_event_categories_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_categories_grouped.document_id_cat\"), \n",
    "                               how='left') \\\n",
    "                         .join(documents_topics_grouped_df \\\n",
    "                                   .withColumnRenamed('topic_id_list', 'doc_event_topic_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_top_list', 'doc_event_confidence_level_top_list') \\\n",
    "                                   .alias('documents_event_topics_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_topics_grouped.document_id_top\"), \n",
    "                               how='left') \\\n",
    "                         .join(documents_entities_grouped_df \\\n",
    "                                   .withColumnRenamed('entity_id_list', 'doc_event_entity_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_ent_list', 'doc_event_confidence_level_ent_list') \\\n",
    "                                   .alias('documents_event_entities_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_entities_grouped.document_id_ent\"), \n",
    "                               how='left') \\\n",
    "                         .join(page_views_users_df, on=[F.col(\"clicks.uuid_event\") == F.col(\"page_views_users.uuid_pv\"),\n",
    "                                                        F.col(\"clicks.document_id_promo\") == F.col(\"page_views_users.document_id_pv\")], \n",
    "                                                  how='left')\n",
    "    \n",
    "    #print(\"validation_set_df.count() =\", validation_set_df.count())\n",
    "        \n",
    "    #Added to validation set information about the event and the user for statistics of the error (avg ctr)\n",
    "    validation_set_ground_truth_df = validation_set_df.filter('clicked = 1') \\\n",
    "                                .join(user_profiles_df, on=[F.col(\"user_profiles.uuid\") == F.col(\"uuid_event\")], how='left') \\\n",
    "                                .withColumn('user_categories_count', list_len_udf('category_id_list')) \\\n",
    "                                .withColumn('user_topics_count', list_len_udf('topic_id_list')) \\\n",
    "                                .withColumn('user_entities_count', list_len_udf('entity_id_list')) \\\n",
    "                                .select('display_id','ad_id','platform_event', 'day_event', 'timestamp_event', \n",
    "                                        'geo_location_event', 'event_country', 'event_country_state', 'views',\n",
    "                                        'user_categories_count', 'user_topics_count', 'user_entities_count') \\\n",
    "                                .withColumnRenamed('ad_id','ad_id_gt') \\\n",
    "                                .withColumnRenamed('views','user_views_count') \\\n",
    "                                .cache()\n",
    "    #print(\"validation_set_ground_truth_df.count() =\", validation_set_ground_truth_df.count())\n",
    "    \n",
    "    train_set_df = spark.sql('''SELECT * FROM clicks_train_joined t \n",
    "                                 WHERE NOT EXISTS (SELECT display_id FROM validation_display_ids \n",
    "                                               WHERE display_id = t.display_id)''').cache()\n",
    "    print(\"train_set_df.count() =\", train_set_df.count())\n",
    "    \n",
    "    #validation_display_ids_df.groupBy(\"day_event\").count().show()\n",
    "    \n",
    "else:\n",
    "    \n",
    "    clicks_test_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"ad_id\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "    clicks_test_df = spark.read.schema(clicks_test_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                    .csv(DATA_BUCKET_FOLDER + \"clicks_test.csv\") \\\n",
    "                    .withColumn('dummyClicksTest', F.lit(1)) \\\n",
    "                    .withColumn('clicked', F.lit(-999)) \\\n",
    "                    .alias('clicks_test')\n",
    "        \n",
    "        \n",
    "    test_set_df = clicks_test_df \\\n",
    "                         .join(promoted_content_df, on='ad_id', how='left') \\\n",
    "                         .join(documents_meta_df, on=F.col(\"promoted_content.document_id_promo\") == F.col(\"documents_meta.document_id_doc\"), how='left') \\\n",
    "                         .join(documents_categories_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_categories_grouped.document_id_cat\"), how='left') \\\n",
    "                         .join(documents_topics_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_topics_grouped.document_id_top\"), how='left') \\\n",
    "                         .join(documents_entities_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_entities_grouped.document_id_ent\"), how='left') \\\n",
    "                         .join(events_joined_df, on='display_id', how='left') \\\n",
    "                         .join(documents_categories_grouped_df \\\n",
    "                                   .withColumnRenamed('category_id_list', 'doc_event_category_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_cat_list', 'doc_event_confidence_level_cat_list') \\\n",
    "                                   .alias('documents_event_categories_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_categories_grouped.document_id_cat\"), \n",
    "                               how='left') \\\n",
    "                         .join(documents_topics_grouped_df \\\n",
    "                                   .withColumnRenamed('topic_id_list', 'doc_event_topic_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_top_list', 'doc_event_confidence_level_top_list') \\\n",
    "                                   .alias('documents_event_topics_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_topics_grouped.document_id_top\"), \n",
    "                               how='left') \\\n",
    "                         .join(documents_entities_grouped_df \\\n",
    "                                   .withColumnRenamed('entity_id_list', 'doc_event_entity_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_ent_list', 'doc_event_confidence_level_ent_list') \\\n",
    "                                   .alias('documents_event_entities_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_entities_grouped.document_id_ent\"), \n",
    "                               how='left') \\\n",
    "                         .join(page_views_users_df, on=[F.col(\"events.uuid_event\") == F.col(\"page_views_users.uuid_pv\"),\n",
    "                                                        F.col(\"promoted_content.document_id_promo\") == F.col(\"page_views_users.document_id_pv\")], \n",
    "                                                  how='left')\n",
    "\n",
    "    #print(\"test_set_df.count() =\",test_set_df.count())\n",
    "   \n",
    "    \n",
    "    train_set_df = clicks_train_joined_df.cache() \n",
    "    print(\"train_set_df.count() =\", train_set_df.count())       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_null(value):\n",
    "    return value == None or len(str(value).strip()) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LESS_SPECIAL_CAT_VALUE = 'less'\n",
    "def get_category_field_values_counts(field, df, min_threshold=10):\n",
    "    category_counts = dict(list(filter(lambda x: not is_null(x[0]) and x[1] >= min_threshold, df.select(field).groupBy(field).count().rdd.map(lambda x: (x[0], x[1])).collect())))\n",
    "    #Adding a special value to create a feature for values in this category that are less than min_threshold \n",
    "    category_counts[LESS_SPECIAL_CAT_VALUE] = -1\n",
    "    return category_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building category values counters and indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_country_values_counts = get_category_field_values_counts('event_country', events_df, min_threshold=10)\n",
    "len(event_country_values_counts)\n",
    "#All non-null categories: 230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1892"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_country_state_values_counts = get_category_field_values_counts('event_country_state', events_df, min_threshold=10)\n",
    "len(event_country_state_values_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2273"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_geo_location_values_counts = get_category_field_values_counts('geo_location_event', events_df, min_threshold=10)\n",
    "len(event_geo_location_values_counts)\n",
    "#All non-null categories: 2988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52439"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_entity_id_values_counts = get_category_field_values_counts('entity_id', documents_entities_df, min_threshold=10)\n",
    "len(doc_entity_id_values_counts)\n",
    "#All non-null categories: 1326009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing average CTR by categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_percentiles(df, field, quantiles_levels=None, max_error_rate=0.0):\n",
    "    if quantiles_levels == None:\n",
    "        quantiles_levels = np.arange(0.0, 1.1, 0.1).tolist() \n",
    "    quantiles = df.approxQuantile(field, quantiles_levels, max_error_rate)\n",
    "    return dict(zip(quantiles_levels, quantiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#REG = 10\n",
    "REG = 0\n",
    "ctr_udf = F.udf(lambda clicks, views: clicks / float(views + REG), FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by ad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ad_id_popularity_df = train_set_df.groupby('ad_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                               F.count('*').alias('views')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ad_id_popularity_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(ad_id_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(ad_id_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ad_id_popularity = ad_id_popularity_df.filter('views > 5').select('ad_id', 'ctr', 'views') \\\n",
    "                    .rdd.map(lambda x: (x['ad_id'], (x['ctr'], x['views'], 1, 1))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ad_id_popularity_broad = sc.broadcast(ad_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.04545454680919647, 22, 1, 1),\n",
       " (0.06832297891378403, 161, 1, 1),\n",
       " (0.125, 32, 1, 1)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ad_id_popularity.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236228"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ad_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_ad_id_ctr_udf = F.udf(lambda ad_id: ad_id_popularity[ad_id] if ad_id in ad_id_popularity else -1, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15279814571842562"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_id_avg_ctr = sum(map(lambda x: x[0], ad_id_popularity.values())) / float(len(ad_id_popularity))\n",
    "ad_id_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19398178324024012"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_id_weighted_avg_ctr = sum(map(lambda x: x[0]*x[1], ad_id_popularity.values())) / float(sum(map(lambda x: x[1], ad_id_popularity.values())))\n",
    "ad_id_weighted_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_id_views_median = np.median(np.array(list(map(lambda x: x[1], ad_id_popularity.values()))))\n",
    "ad_id_views_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366.68235348900214"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_id_views_mean = sum(map(lambda x: x[1], ad_id_popularity.values())) / float(len(ad_id_popularity))\n",
    "ad_id_views_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by document_id (promoted_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88857"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_popularity_df = train_set_df.groupby('document_id_promo').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                               F.count('*').alias('views'),\n",
    "                                                               F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "    \n",
    "document_id_popularity = document_id_popularity_df.filter('views > 5').select('document_id_promo', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "                                                .rdd.map(lambda x: (x['document_id_promo'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(document_id_popularity)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document_id_popularity_broad = sc.broadcast(document_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#document_id_popularity_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(document_id_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(document_id_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1465989063948768"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_avg_ctr = sum(map(lambda x: x[0], document_id_popularity.values())) / float(len(document_id_popularity))\n",
    "document_id_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1937924606865621"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_weighted_avg_ctr = sum(list(map(lambda x: x[0]*x[1], document_id_popularity.values()))) / float(sum(list(map(lambda x: x[1], document_id_popularity.values()))))\n",
    "document_id_weighted_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_views_median = np.median(np.array(list(map(lambda x: x[1], document_id_popularity.values()))))\n",
    "document_id_views_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "978.9117570928571"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_views_mean = sum(map(lambda x: x[1], document_id_popularity.values())) / float(len(document_id_popularity))\n",
    "document_id_views_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by (doc_event, doc_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1902834"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_event_doc_ad_avg_ctr_df = train_set_df.groupBy('document_id_event', 'document_id_promo') \\\n",
    "                                    .agg(F.sum('clicked').alias('clicks'), \n",
    "                                         F.count('*').alias('views'),\n",
    "                                         F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                    .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "doc_event_doc_ad_avg_ctr = doc_event_doc_ad_avg_ctr_df.filter('views > 5') \\\n",
    "                    .select('document_id_event', 'document_id_promo','ctr', 'views', 'distinct_ad_ids') \\\n",
    "                    .rdd.map(lambda x: ((x['document_id_event'], x['document_id_promo']), (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()        \n",
    "\n",
    "len(doc_event_doc_ad_avg_ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_event_doc_ad_avg_ctr_broad = sc.broadcast(doc_event_doc_ad_avg_ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by country, source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34333"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_popularity_df = train_set_df.select('clicked', 'source_id', 'event_country', 'ad_id') \\\n",
    "                                            .groupby('event_country', 'source_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                             F.count('*').alias('views'),\n",
    "                                                                             F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "#source_id_popularity = source_id_popularity_df.filter('views > 100 and source_id is not null').select('source_id', 'ctr').rdd.collectAsMap()\n",
    "source_id_by_country_popularity = source_id_by_country_popularity_df.filter('views > 5 and source_id is not null and event_country <> \"\"').select('event_country', 'source_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "        .rdd.map(lambda x: ((x['event_country'], x['source_id']), (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(source_id_by_country_popularity)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_id_by_country_popularity_broad = sc.broadcast(source_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1846588083811812"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_avg_ctr = sum(map(lambda x: x[0], source_id_by_country_popularity.values())) / float(len(source_id_by_country_popularity))\n",
    "source_id_by_country_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19367626877411454"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_weighted_avg_ctr = sum(map(lambda x: x[0]*x[1], source_id_by_country_popularity.values())) / float(sum(map(lambda x: x[1], source_id_by_country_popularity.values())))\n",
    "source_id_by_country_weighted_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_views_median = np.median(np.array(list(map(lambda x: x[1], source_id_by_country_popularity.values()))))\n",
    "source_id_by_country_views_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2534.7824833250806"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_views_mean = sum(map(lambda x: x[1], source_id_by_country_popularity.values())) / float(len(source_id_by_country_popularity))\n",
    "source_id_by_country_views_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5855"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_popularity_df = train_set_df.select('clicked', 'source_id', 'ad_id') \\\n",
    "                                            .groupby('source_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                     F.count('*').alias('views'),\n",
    "                                                                     F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "source_id_popularity = source_id_popularity_df.filter('views > 10 and source_id is not null').select('source_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['source_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(source_id_popularity)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_id_popularity_broad = sc.broadcast(source_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#source_id_popularity_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(source_id_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(source_id_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#source_id_popularity = source_id_popularity_df.filter('views > 100 and source_id is not null').select('source_id', 'ctr').rdd.collectAsMap()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by publisher_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publisher_popularity_df = train_set_df.select('clicked', 'publisher_id', 'ad_id') \\\n",
    "                                            .groupby('publisher_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                             F.count('*').alias('views'),\n",
    "                                                                              F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "publisher_popularity = publisher_popularity_df.filter('views > 10 and publisher_id is not null').select('publisher_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['publisher_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(publisher_popularity)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "publisher_popularity_broad = sc.broadcast(publisher_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#publisher_popularity_df.count()\n",
    "##863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(publisher_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(publisher_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#publisher_id_popularity = publisher_popularity_df.filter('views > 100 and publisher_id is not null').select('publisher_id', 'ctr').rdd.collectAsMap()\n",
    "#len(publisher_id_popularity)\n",
    "##639"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by advertiser_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3731"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advertiser_id_popularity_df = train_set_df.select('clicked', 'advertiser_id', 'ad_id') \\\n",
    "                                            .groupby('advertiser_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                          F.count('*').alias('views'),\n",
    "                                                                          F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "advertiser_id_popularity = advertiser_id_popularity_df.filter('views > 10 and advertiser_id is not null').select('advertiser_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['advertiser_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(advertiser_id_popularity)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "advertiser_id_popularity_broad = sc.broadcast(advertiser_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#advertiser_id_popularity_df.count()\n",
    "##4063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(advertiser_id_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(advertiser_id_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#advertiser_id_popularity = advertiser_id_popularity_df.filter('views > 100 and advertiser_id is not null').select('advertiser_id', 'ctr').rdd.collectAsMap()\n",
    "#len(advertiser_id_popularity)\n",
    "##3129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by campaign_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26965"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "campaign_id_popularity_df = train_set_df.select('clicked', 'campaign_id', 'ad_id') \\\n",
    "                                            .groupby('campaign_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                        F.count('*').alias('views'),\n",
    "                                                                        F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "campaign_id_popularity = campaign_id_popularity_df.filter('views > 10 and campaign_id is not null').select('campaign_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['campaign_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(campaign_id_popularity)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "campaign_id_popularity_broad = sc.broadcast(campaign_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#campaign_id_popularity_df.count()\n",
    "##31390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(campaign_id_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(campaign_id_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#campaign_id_popularity = campaign_id_popularity_df.filter('views > 100 and campaign_id is not null').select('campaign_id', 'ctr').rdd.collectAsMap()\n",
    "#len(campaign_id_popularity)\n",
    "##16097"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "category_id_popularity_df = train_set_df.join(documents_categories_df.alias('cat_local'), on=F.col(\"document_id_promo\") == F.col(\"cat_local.document_id_cat\"), how='inner') \\\n",
    "                                        .select('clicked', 'category_id', 'confidence_level_cat', 'ad_id') \\\n",
    "                                        .groupby('category_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                    F.count('*').alias('views'),\n",
    "                                                                    F.mean('confidence_level_cat').alias('avg_confidence_level_cat'),\n",
    "                                                                    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "            \n",
    "category_id_popularity = category_id_popularity_df.filter('views > 10').select('category_id', 'ctr', 'views', 'avg_confidence_level_cat', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['category_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_cat']))).collectAsMap()\n",
    "len(category_id_popularity)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_id_popularity_broad = sc.broadcast(category_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.22030237317085266, 1389, 11, 0.20000000298023224),\n",
       " (0.17162761092185974, 1228899, 8053, 0.08850012670593373),\n",
       " (0.10092294961214066, 199578, 801, 0.27195677871997787),\n",
       " (0.20434629917144775, 1876173, 13519, 0.6272726974344387),\n",
       " (0.2549199163913727, 898133, 9280, 0.3348816519177221),\n",
       " (0.17458122968673706, 509946, 4833, 0.369294788638708),\n",
       " (0.21058064699172974, 3170847, 19549, 0.5149971063427476),\n",
       " (0.19907772541046143, 1293314, 6781, 0.2812444300215735),\n",
       " (0.21065735816955566, 1924229, 9923, 0.47779856930040854),\n",
       " (0.1487119048833847, 453111, 2187, 0.3726597747463394)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(category_id_popularity.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1008536.0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(np.array(list(map(lambda x: x[1], category_id_popularity.values()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1816819.5894736843"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda x: x[1], category_id_popularity.values())) / float(len(category_id_popularity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parece haver uma hierarquia nas categorias pelo padrão dos códigos...\n",
    "#category_id_popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by (country, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11910"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_id_by_country_popularity_df = train_set_df.join(documents_categories_df.alias('cat_local'), on=F.col(\"document_id_promo\") == F.col(\"cat_local.document_id_cat\"), how='inner') \\\n",
    "                                        .select('clicked', 'category_id', 'confidence_level_cat', 'event_country', 'ad_id') \\\n",
    "                                        .groupby('event_country','category_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                                    F.count('*').alias('views'),\n",
    "                                                                                    F.mean('confidence_level_cat').alias('avg_confidence_level_cat'),\n",
    "                                                                                    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "\n",
    "category_id_by_country_popularity = category_id_by_country_popularity_df.filter('views > 10 and event_country <> \"\"').select('event_country', 'category_id', 'ctr', 'views', 'avg_confidence_level_cat', 'distinct_ad_ids') \\\n",
    "                                     .rdd.map(lambda x: ((x['event_country'], x['category_id']), (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_cat']))).collectAsMap()\n",
    "len(category_id_by_country_popularity)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_id_by_country_popularity_broad = sc.broadcast(category_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_id_popularity_df = train_set_df.join(documents_topics_df.alias('top_local'), on=F.col(\"document_id_promo\") == F.col(\"top_local.document_id_top\"), how='inner') \\\n",
    "                                        .select('clicked', 'topic_id', 'confidence_level_top', 'ad_id') \\\n",
    "                                        .groupby('topic_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                 F.count('*').alias('views'),\n",
    "                                                                 F.mean('confidence_level_top').alias('avg_confidence_level_top'),\n",
    "                                                                 F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "topic_id_popularity = topic_id_popularity_df.filter('views > 10').select('topic_id', 'ctr', 'views', 'avg_confidence_level_top', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['topic_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_top']))).collectAsMap()\n",
    "len(topic_id_popularity)                                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_id_popularity_broad = sc.broadcast(topic_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768711.9333333333"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda x: x[1], topic_id_popularity.values())) / float(len(topic_id_popularity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11640508052.943333"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda x: x[2]*x[1], topic_id_popularity.values())) / float(len(topic_id_popularity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by (country, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36611"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_id_by_country_popularity_df = train_set_df.join(documents_topics_df.alias('top_local'), on=F.col(\"document_id_promo\") == F.col(\"top_local.document_id_top\"), how='inner') \\\n",
    "                                        .select('clicked', 'topic_id', 'confidence_level_top','event_country', 'ad_id') \\\n",
    "                                        .groupby('event_country','topic_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                             F.count('*').alias('views'),\n",
    "                                                                             F.mean('confidence_level_top').alias('avg_confidence_level_top'),\n",
    "                                                                             F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "            \n",
    "topic_id_id_by_country_popularity = topic_id_by_country_popularity_df.filter('views > 10 and event_country <> \"\"').select('event_country', 'topic_id', 'ctr', 'views', 'avg_confidence_level_top', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: ((x['event_country'], x['topic_id']), (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_top']))).collectAsMap()\n",
    "len(topic_id_id_by_country_popularity)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_id_id_by_country_popularity_broad = sc.broadcast(topic_id_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89004"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_id_popularity_df = train_set_df.join(documents_entities_df.alias('ent_local'), on=F.col(\"document_id_promo\") == F.col(\"ent_local.document_id_ent\"), how='inner') \\\n",
    "                                        .select('clicked', 'entity_id', 'confidence_level_ent', 'ad_id') \\\n",
    "                                        .groupby('entity_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                  F.count('*').alias('views'),\n",
    "                                                                  F.mean('confidence_level_ent').alias('avg_confidence_level_ent'),\n",
    "                                                                  F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "\n",
    "entity_id_popularity = entity_id_popularity_df.filter('views > 5').select('entity_id', 'ctr', 'views', 'avg_confidence_level_ent', 'distinct_ad_ids') \\\n",
    "                                     .rdd.map(lambda x: (x['entity_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_ent']))).collectAsMap()\n",
    "len(entity_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity_id_popularity_broad = sc.broadcast(entity_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(np.array(list(map(lambda x: x[1], entity_id_popularity.values()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2449.352096534987"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda x: x[1], entity_id_popularity.values())) / float(len(entity_id_popularity))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by (country, entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262920"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_id_by_country_popularity_df = train_set_df.join(documents_entities_df.alias('ent_local'), on=F.col(\"document_id_promo\") == F.col(\"ent_local.document_id_ent\"), how='inner') \\\n",
    "                                        .select('clicked', 'entity_id', 'event_country', 'confidence_level_ent','ad_id') \\\n",
    "                                        .groupby('event_country','entity_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                             F.count('*').alias('views'),\n",
    "                                                                             F.mean('confidence_level_ent').alias('avg_confidence_level_ent'),\n",
    "                                                                             F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "            \n",
    "entity_id_by_country_popularity = entity_id_by_country_popularity_df.filter('views > 5 and event_country <> \"\"').select('event_country', 'entity_id', 'ctr', 'views', 'avg_confidence_level_ent', 'distinct_ad_ids') \\\n",
    "                .rdd.map(lambda x: ((x['event_country'], x['entity_id']), (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_ent']))).collectAsMap()\n",
    "len(entity_id_by_country_popularity)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity_id_by_country_popularity_broad = sc.broadcast(entity_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading # docs by categories, topics, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import cPickle\n",
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_filenames_suffix = ''\n",
    "if evaluation:\n",
    "    df_filenames_suffix = '_eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('aux_data/categories_docs_counts'+df_filenames_suffix+'.pickle', 'rb') as input_file:\n",
    "    categories_docs_counts = cPickle.load(input_file)    \n",
    "len(categories_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('aux_data/topics_docs_counts'+df_filenames_suffix+'.pickle', 'rb') as input_file:\n",
    "    topics_docs_counts = cPickle.load(input_file)\n",
    "len(topics_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1326009"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('aux_data/entities_docs_counts'+df_filenames_suffix+'.pickle', 'rb') as input_file:\n",
    "    entities_docs_counts = cPickle.load(input_file)\n",
    "len(entities_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999334"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_total = documents_meta_df.count()\n",
    "documents_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Publish Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.5: 1464256800.0}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publish_times_df = train_set_df.filter('publish_time is not null').select('document_id_promo','publish_time').distinct().select(F.col('publish_time').cast(IntegerType()))\n",
    "publish_time_percentiles = get_percentiles(publish_times_df, 'publish_time', quantiles_levels=[0.5], max_error_rate=0.001)\n",
    "publish_time_percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 5, 26, 10, 0)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publish_time_median = int(publish_time_percentiles[0.5])\n",
    "datetime.datetime.utcfromtimestamp(publish_time_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_days_diff(newer_timestamp, older_timestamp):\n",
    "    sec_diff = newer_timestamp - older_timestamp\n",
    "    days_diff = sec_diff / 60 / 60 / 24\n",
    "    return days_diff\n",
    "\n",
    "def get_time_decay_factor(timestamp, timestamp_ref=None, alpha=0.001):\n",
    "    if timestamp_ref == None:\n",
    "        timestamp_ref = time.time()\n",
    "        \n",
    "    days_diff = get_days_diff(timestamp_ref, timestamp)\n",
    "    denominator = math.pow(1+alpha, days_diff)\n",
    "    if denominator != 0:\n",
    "        return 1.0 / denominator\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_odd_timestamp(timestamp_ms_relative):\n",
    "    TIMESTAMP_DELTA=1465876799998\n",
    "    return datetime.datetime.fromtimestamp((int(timestamp_ms_relative)+TIMESTAMP_DELTA)//1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TIME_DECAY_ALPHA = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-17 14:34:40 0.8944377229295418\n",
      "2016-09-24 14:34:40 0.8842131490002618\n",
      "2016-07-24 14:34:40 0.8572296906380295\n",
      "2016-04-24 14:34:40 0.8191090875190835\n",
      "2015-10-24 14:34:40 0.747504360246784\n",
      "2014-10-24 14:34:40 0.622837561073834\n"
     ]
    }
   ],
   "source": [
    "ref_dates = [\n",
    "                1476714880, # 7 days\n",
    "                1474727680, # 30 days\n",
    "                1469370880, # 90 days\n",
    "                1461508480,  # 180 days\n",
    "                1445697280, # 1 year\n",
    "                1414161280 # 2 years\n",
    "]\n",
    "\n",
    "for d in ref_dates:\n",
    "    print(datetime.datetime.utcfromtimestamp(d), get_time_decay_factor(d, alpha=TIME_DECAY_ALPHA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get local time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEFAULT_TZ_EST = -4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_local_utc_bst_tz(event_country, event_country_state):\n",
    "    local_tz = DEFAULT_TZ_EST\n",
    "    if len(event_country) > 0:\n",
    "        if event_country in countries_utc_dst_broad.value:\n",
    "            local_tz = countries_utc_dst_broad.value[event_country]\n",
    "            if len(event_country_state)>2:\n",
    "                state = event_country_state[3:5]\n",
    "                if event_country == 'US':  \n",
    "                    if state in us_states_utc_dst_broad.value:\n",
    "                        local_tz = us_states_utc_dst_broad.value[state]                \n",
    "                elif event_country == 'CA':\n",
    "                    if state in ca_countries_utc_dst_broad.value:\n",
    "                        local_tz = ca_countries_utc_dst_broad.value[state] \n",
    "    return float(local_tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hour_bins_dict = {'EARLY_MORNING': 1,\n",
    "             'MORNING': 2,\n",
    "             'MIDDAY': 3,\n",
    "             'AFTERNOON': 4,\n",
    "             'EVENING': 5,\n",
    "             'NIGHT': 6}\n",
    "\n",
    "hour_bins_values = sorted(hour_bins_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hour_bin(hour):\n",
    "    if hour >= 5 and hour < 8:\n",
    "        hour_bin = hour_bins_dict['EARLY_MORNING']\n",
    "    elif hour >= 8 and hour < 11:\n",
    "        hour_bin = hour_bins_dict['MORNING']\n",
    "    elif hour >= 11 and hour < 14:\n",
    "        hour_bin = hour_bins_dict['MIDDAY']\n",
    "    elif hour >= 14 and hour < 19:\n",
    "        hour_bin = hour_bins_dict['AFTERNOON']\n",
    "    elif hour >= 19 and hour < 22:\n",
    "        hour_bin = hour_bins_dict['EVENING']\n",
    "    else:\n",
    "        hour_bin = hour_bins_dict['NIGHT']\n",
    "    return hour_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_local_datetime(dt, event_country, event_country_state):\n",
    "    local_tz = get_local_utc_bst_tz(event_country, event_country_state)  \n",
    "    tz_delta = local_tz - DEFAULT_TZ_EST\n",
    "    local_time = dt +  datetime.timedelta(hours=tz_delta)\n",
    "    return local_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2017, 5, 28, 15, 49, 32, 782115)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_local_datetime(datetime.datetime.now(), 'US', 'US>CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_weekend(dt):\n",
    "    return dt.weekday() >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_weekend(datetime.datetime(2016, 6, 14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CTR functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decay_factor_default 0.9832707377478735\n"
     ]
    }
   ],
   "source": [
    "timestamp_ref = date_time_to_unix_epoch(datetime.datetime(2016, 6, 29, 3, 59, 59))\n",
    "decay_factor_default = get_time_decay_factor(publish_time_median, timestamp_ref, alpha=TIME_DECAY_ALPHA)\n",
    "print(\"decay_factor_default\", decay_factor_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "0.5 0.024411410743763327\n",
      "1 0.041731582304281624\n",
      "2 0.06614299304804495\n",
      "3 0.08346316460856325\n",
      "4 0.09689773339641579\n",
      "5 0.10787457535232657\n",
      "10 0.14436755531919657\n",
      "20 0.183298356035222\n",
      "30 0.20674645107847822\n",
      "100 0.2778577004917695\n",
      "200 0.3192904933647466\n",
      "300 0.34360197720285013\n",
      "1000 0.41594812296601125\n",
      "2000 0.4576496248565576\n",
      "3000 0.48205100545505175\n",
      "10000 0.5545232830964639\n",
      "20000 0.5962518553291584\n",
      "30000 0.6206622626822822\n",
      "50000 0.6514162003061013\n",
      "90000 0.6868039178501281\n",
      "100000 1.0\n",
      "500000 1.0\n",
      "900000 1.0\n",
      "1000000 1.0\n",
      "2171607 1.0\n"
     ]
    }
   ],
   "source": [
    "def get_confidence_sample_size(sample, max_for_reference=100000):\n",
    "    #Avoiding overflow for large sample size\n",
    "    if sample >= max_for_reference:\n",
    "        return 1.0\n",
    "\n",
    "    ref_log = math.log(1+max_for_reference, 2) #Curiosly reference in log  with base 2 gives a slightly higher score, so I will keep\n",
    "    \n",
    "    return math.log(1+sample) / float(ref_log)\n",
    "    \n",
    "for i in [0,0.5,1,2,3,4,5,10,20,30,100,200,300,1000,2000,3000,10000,20000,30000, 50000, 90000, 100000, 500000, 900000, 1000000, 2171607]:\n",
    "    print(i, get_confidence_sample_size(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_popularity(an_id, a_dict):\n",
    "    return (a_dict[an_id][0], get_confidence_sample_size(a_dict[an_id][1] / float(a_dict[an_id][2])) * a_dict[an_id][3]) if an_id in a_dict else (None, None)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_avg_popularity_from_list(ids_list, confidence_ids_list, pop_dict):\n",
    "    pops = list(filter(lambda x: x[0][0]!=None, [(get_popularity(an_id, pop_dict), confidence) for an_id, confidence in zip(ids_list, confidence_ids_list)]))\n",
    "    #print(\"pops\",pops)\n",
    "    if len(pops) > 0:\n",
    "        weighted_avg = sum(map(lambda x: x[0][0]*x[0][1]*x[1], pops)) / float(sum(map(lambda x: x[0][1]*x[1], pops)))\n",
    "        confidence = max(map(lambda x: x[0][1]*x[1], pops))\n",
    "        return weighted_avg, confidence\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_avg_country_popularity_from_list(event_country, ids_list, confidence_ids_list, pop_dict):\n",
    "    pops = list(filter(lambda x: x[0][0]!=None, [(get_popularity((event_country, an_id), pop_dict), confidence) for an_id, confidence in zip(ids_list, confidence_ids_list)]))\n",
    "    \n",
    "    if len(pops) > 0:\n",
    "        weighted_avg = sum(map(lambda x: x[0][0]*x[0][1]*x[1], pops)) / float(sum(map(lambda x: x[0][1]*x[1], pops)))\n",
    "        confidence = max(map(lambda x: x[0][1]*x[1], pops))\n",
    "        return weighted_avg, confidence\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_popularity_score(event_country, ad_id, document_id, source_id, \n",
    "                         publisher_id, advertiser_id, campaign_id, document_id_event,\n",
    "                            category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                            topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                            entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "                            output_detailed_list=False):\n",
    "    probs = []\n",
    "    \n",
    "    avg_ctr, confidence = get_popularity(ad_id, ad_id_popularity_broad.value)    \n",
    "    if avg_ctr != None:\n",
    "        probs.append(('pop_ad_id', avg_ctr, confidence))\n",
    "        \n",
    "    avg_ctr, confidence = get_popularity(document_id, document_id_popularity_broad.value)\n",
    "    if avg_ctr != None:\n",
    "        probs.append(('pop_document_id', avg_ctr, confidence))  \n",
    "        \n",
    "    avg_ctr, confidence = get_popularity((document_id_event, document_id), doc_event_doc_ad_avg_ctr_broad.value)\n",
    "    if avg_ctr != None:\n",
    "        probs.append(('pop_doc_event_doc_ad', avg_ctr, confidence))\n",
    "        \n",
    "        \n",
    "    if source_id != -1:\n",
    "        avg_ctr = None\n",
    "        if event_country != '':\n",
    "            avg_ctr, confidence = get_popularity((event_country, source_id), source_id_by_country_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_source_id_country', avg_ctr, confidence))\n",
    "            \n",
    "        avg_ctr, confidence = get_popularity(source_id, source_id_popularity_broad.value)        \n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_source_id', avg_ctr, confidence))\n",
    "            \n",
    "            \n",
    "    if publisher_id != None:\n",
    "        avg_ctr, confidence = get_popularity(publisher_id, publisher_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_publisher_id', avg_ctr, confidence)) \n",
    "            \n",
    "    if advertiser_id != None:\n",
    "        avg_ctr, confidence = get_popularity(advertiser_id, advertiser_id_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_advertiser_id', avg_ctr, confidence)) \n",
    "    \n",
    "    if campaign_id != None:\n",
    "        avg_ctr, confidence = get_popularity(campaign_id, campaign_id_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_campain_id', avg_ctr, confidence))  \n",
    "\n",
    "    if len(entity_ids_by_doc) > 0: \n",
    "        avg_ctr = None\n",
    "        if event_country != '':\n",
    "            avg_ctr, confidence = get_weighted_avg_country_popularity_from_list(event_country, entity_ids_by_doc, ent_confidence_level_by_doc, \n",
    "                                        entity_id_by_country_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_entity_id_country', avg_ctr, confidence))\n",
    "            \n",
    "        avg_ctr, confidence = get_weighted_avg_popularity_from_list(entity_ids_by_doc, ent_confidence_level_by_doc, \n",
    "                                                                    entity_id_popularity_broad.value) \n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_entity_id', avg_ctr, confidence))\n",
    "            \n",
    "    \n",
    "    \n",
    "    if len(topic_ids_by_doc) > 0:  \n",
    "        avg_ctr = None\n",
    "        if event_country != '':\n",
    "            avg_ctr, confidence = get_weighted_avg_country_popularity_from_list(event_country, topic_ids_by_doc, top_confidence_level_by_doc, \n",
    "                                        topic_id_id_by_country_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_topic_id_country', avg_ctr, confidence))\n",
    "            \n",
    "        avg_ctr, confidence = get_weighted_avg_popularity_from_list(topic_ids_by_doc, top_confidence_level_by_doc, \n",
    "                                                                    topic_id_popularity_broad.value)            \n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_topic_id', avg_ctr, confidence))\n",
    "    \n",
    "    \n",
    "    if len(category_ids_by_doc) > 0:  \n",
    "        avg_ctr = None\n",
    "        if event_country != '':\n",
    "            avg_ctr, confidence = get_weighted_avg_country_popularity_from_list(event_country, category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                        category_id_by_country_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_category_id_country', avg_ctr, confidence))\n",
    "        \n",
    "        avg_ctr, confidence = get_weighted_avg_popularity_from_list(category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                                                    category_id_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_category_id', avg_ctr, confidence))\n",
    "    \n",
    "    #print(\"[get_popularity_score] probs\", probs)\n",
    "    if output_detailed_list:\n",
    "        return probs\n",
    "    \n",
    "    else:    \n",
    "        if len(probs) > 0:\n",
    "            #weighted_avg_probs_by_confidence = sum(map(lambda x: x[1] *  math.log(1+x[2],2), probs)) / float(sum(map(lambda x: math.log(1+x[2],2), probs)))        \n",
    "            weighted_avg_probs_by_confidence = sum(map(lambda x: x[1] * x[2], probs)) / float(sum(map(lambda x: x[2], probs)))                \n",
    "            confidence = max(map(lambda x: x[2], probs))\n",
    "            return weighted_avg_probs_by_confidence, confidence\n",
    "        else:\n",
    "            return None, None    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based similarity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_dicts(dict1, dict2):\n",
    "    dict1_norm = math.sqrt(sum([v**2 for v in dict1.values()]))\n",
    "    dict2_norm = math.sqrt(sum([v**2 for v in dict2.values()]))\n",
    "    \n",
    "    sum_common_aspects = 0.0\n",
    "    intersections = 0\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            sum_common_aspects += dict1[key] * dict2[key] \n",
    "            intersections += 1\n",
    "        \n",
    "    return sum_common_aspects / (dict1_norm * dict2_norm), intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_user_docs_aspects(user_aspect_profile, doc_aspect_ids, doc_aspects_confidence, aspect_docs_counts):\n",
    "    if user_aspect_profile==None or len(user_aspect_profile) == 0 or doc_aspect_ids == None or len(doc_aspect_ids) == 0:\n",
    "        return None, None\n",
    "        \n",
    "    doc_aspects = dict(zip(doc_aspect_ids, doc_aspects_confidence))\n",
    "    doc_aspects_tfidf_confid = {}\n",
    "    for key in doc_aspects:\n",
    "        tf = 1.0\n",
    "        idf = math.log(math.log(documents_total / float(aspect_docs_counts[key])))\n",
    "        confidence = doc_aspects[key]\n",
    "        doc_aspects_tfidf_confid[key] = tf*idf * confidence\n",
    "        \n",
    "    user_aspects_tfidf_confid = {}    \n",
    "    for key in user_aspect_profile:\n",
    "        tfidf = user_aspect_profile[key][0]\n",
    "        confidence = user_aspect_profile[key][1]\n",
    "        user_aspects_tfidf_confid[key] = tfidf * confidence\n",
    "        \n",
    "    similarity, intersections = cosine_similarity_dicts(doc_aspects_tfidf_confid, user_aspects_tfidf_confid)\n",
    "    \n",
    "    if intersections > 0:\n",
    "        #P(A intersect B)_intersections = P(A)^intersections * P(B)^intersections\n",
    "        random_error = math.pow(len(doc_aspects)         / float(len(aspect_docs_counts)), intersections) * \\\n",
    "                       math.pow(len(user_aspect_profile) / float(len(aspect_docs_counts)), intersections)\n",
    "        confidence = 1.0 - random_error\n",
    "    else:\n",
    "        #P(A not intersect B) = 1 - P(A intersect B)\n",
    "        random_error = 1 - ((len(doc_aspects) / float(len(aspect_docs_counts))) * \\\n",
    "                            (len(user_aspect_profile) / float(len(aspect_docs_counts))))\n",
    "    \n",
    "    confidence = 1.0 - random_error    \n",
    "    \n",
    "    return similarity, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_doc_event_doc_ad_aspects(doc_event_aspect_ids, doc_event_aspects_confidence, \n",
    "                                               doc_ad_aspect_ids, doc_ad_aspects_confidence, \n",
    "                                               aspect_docs_counts):\n",
    "    if doc_event_aspect_ids == None or len(doc_event_aspect_ids) == 0 or \\\n",
    "       doc_ad_aspect_ids == None or len(doc_ad_aspect_ids) == 0:\n",
    "        return None, None\n",
    "        \n",
    "    doc_event_aspects = dict(zip(doc_event_aspect_ids, doc_event_aspects_confidence))\n",
    "    doc_event_aspects_tfidf_confid = {}\n",
    "    for key in doc_event_aspect_ids:\n",
    "        tf = 1.0\n",
    "        idf = math.log(math.log(documents_total / float(aspect_docs_counts[key])))\n",
    "        confidence = doc_event_aspects[key]\n",
    "        doc_event_aspects_tfidf_confid[key] = tf*idf * confidence\n",
    "        \n",
    "    doc_ad_aspects = dict(zip(doc_ad_aspect_ids, doc_ad_aspects_confidence))\n",
    "    doc_ad_aspects_tfidf_confid = {}\n",
    "    for key in doc_ad_aspect_ids:\n",
    "        tf = 1.0\n",
    "        idf = math.log(math.log(documents_total / float(aspect_docs_counts[key])))\n",
    "        confidence = doc_ad_aspects[key]\n",
    "        doc_ad_aspects_tfidf_confid[key] = tf*idf * confidence\n",
    "        \n",
    "    similarity, intersections = cosine_similarity_dicts(doc_event_aspects_tfidf_confid, doc_ad_aspects_tfidf_confid)\n",
    "    \n",
    "    if intersections > 0:\n",
    "        #P(A intersect B)_intersections = P(A)^intersections * P(B)^intersections\n",
    "        random_error = math.pow(len(doc_event_aspect_ids) / float(len(aspect_docs_counts)), intersections) * \\\n",
    "                       math.pow(len(doc_ad_aspect_ids) / float(len(aspect_docs_counts)), intersections)\n",
    "        confidence = 1.0 - random_error\n",
    "    else:\n",
    "        #P(A not intersect B) = 1 - P(A intersect B)\n",
    "        random_error = 1 - ((len(doc_event_aspect_ids) / float(len(aspect_docs_counts))) * \\\n",
    "                            (len(doc_ad_aspect_ids) / float(len(aspect_docs_counts))))\n",
    "    \n",
    "    confidence = 1.0 - random_error    \n",
    "    \n",
    "    return similarity, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_cb_interest_score(user_views_count, user_categories, user_topics, user_entities, \n",
    "                            timestamp_event, category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                            topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                            entity_ids_by_doc, ent_confidence_level_by_doc, \n",
    "                            output_detailed_list=False):\n",
    "\n",
    "    #Content-Based\n",
    "    \n",
    "    sims = []\n",
    "    \n",
    "    categories_similarity, cat_sim_confidence = cosine_similarity_user_docs_aspects(user_categories, category_ids_by_doc, cat_confidence_level_by_doc, categories_docs_counts)\n",
    "    if categories_similarity != None:\n",
    "        sims.append(('user_doc_ad_sim_categories', categories_similarity, cat_sim_confidence))\n",
    "    \n",
    "    topics_similarity, top_sim_confidence = cosine_similarity_user_docs_aspects(user_topics, topic_ids_by_doc, top_confidence_level_by_doc, topics_docs_counts)\n",
    "    if topics_similarity != None:\n",
    "        sims.append(('user_doc_ad_sim_topics', topics_similarity, top_sim_confidence))\n",
    "    \n",
    "    entities_similarity, entity_sim_confid = cosine_similarity_user_docs_aspects(user_entities, entity_ids_by_doc, ent_confidence_level_by_doc, entities_docs_counts)\n",
    "    if entities_similarity != None:\n",
    "        sims.append(('user_doc_ad_sim_entities', entities_similarity, entity_sim_confid))\n",
    "    \n",
    "    if output_detailed_list:\n",
    "        return sims\n",
    "    else:\n",
    "        if len(sims) > 0:\n",
    "            weighted_avg_sim_by_confidence = sum(map(lambda x: x[1]*x[2], sims)) / float(sum(map(lambda x: x[2], sims)))\n",
    "            confidence = sum(map(lambda x: x[2], sims)) / float(len(sims))\n",
    "\n",
    "            #print(\"[get_user_cb_interest_score] sims: {} | Avg: {} - Confid: {}\".format(sims, weighted_avg_sim_by_confidence, confidence))\n",
    "            return weighted_avg_sim_by_confidence, confidence\n",
    "        else:\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_doc_event_doc_ad_cb_similarity_score(doc_event_category_ids, doc_event_cat_confidence_levels, \n",
    "                                             doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                                             doc_event_entity_ids, doc_event_ent_confidence_levels, \n",
    "                                             doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                             doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                             doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                            output_detailed_list=False):\n",
    "\n",
    "    #Content-Based\n",
    "    sims = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    categories_similarity, cat_sim_confidence = cosine_similarity_doc_event_doc_ad_aspects(\n",
    "                                                    doc_event_category_ids, doc_event_cat_confidence_levels, \n",
    "                                                    doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                                    categories_docs_counts)\n",
    "    if categories_similarity != None:\n",
    "        sims.append(('doc_event_doc_ad_sim_categories', categories_similarity, cat_sim_confidence))\n",
    "    \n",
    "    topics_similarity, top_sim_confidence = cosine_similarity_doc_event_doc_ad_aspects(\n",
    "                                                    doc_event_topic_ids, doc_event_top_confidence_levels, \n",
    "                                                    doc_ad_topic_ids, doc_ad_top_confidence_levels, \n",
    "                                                    topics_docs_counts)\n",
    "    \n",
    "    if topics_similarity != None:\n",
    "        sims.append(('doc_event_doc_ad_sim_topics', topics_similarity, top_sim_confidence))\n",
    "        \n",
    "    entities_similarity, entity_sim_confid = cosine_similarity_doc_event_doc_ad_aspects(\n",
    "                                                    doc_event_entity_ids, doc_event_ent_confidence_levels, \n",
    "                                                    doc_ad_entity_ids, doc_ad_ent_confidence_levels, \n",
    "                                                    entities_docs_counts)\n",
    "    \n",
    "    if entities_similarity != None:\n",
    "        sims.append(('doc_event_doc_ad_sim_entities', entities_similarity, entity_sim_confid))\n",
    "    \n",
    "    if output_detailed_list:\n",
    "        return sims\n",
    "    else:\n",
    "        if len(sims) > 0:\n",
    "            weighted_avg_sim_by_confidence = sum(map(lambda x: x[1]*x[2], sims)) / float(sum(map(lambda x: x[2], sims)))\n",
    "            confidence = sum(map(lambda x: x[2], sims)) / float(len(sims))\n",
    "\n",
    "            #print(\"[get_user_cb_interest_score] sims: {} | Avg: {} - Confid: {}\".format(sims, weighted_avg_sim_by_confidence, confidence))\n",
    "            return weighted_avg_sim_by_confidence, confidence\n",
    "        else:\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Vector export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bool_feature_names = ['event_weekend',\n",
    "                      'user_has_already_viewed_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_feature_names = ['user_views',\n",
    "                    'ad_views',\n",
    "                    'doc_views',\n",
    "                    'doc_event_days_since_published',\n",
    "                    'doc_event_hour',\n",
    "                    'doc_ad_days_since_published', \n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "float_feature_names = [                                \n",
    "                'pop_ad_id',       \n",
    "                'pop_ad_id_conf',   \n",
    "                'pop_ad_id_conf_multipl', \n",
    "                'pop_document_id',                \n",
    "                'pop_document_id_conf',\n",
    "                'pop_document_id_conf_multipl',\n",
    "                'pop_publisher_id',\n",
    "                'pop_publisher_id_conf',\n",
    "                'pop_publisher_id_conf_multipl',\n",
    "                'pop_advertiser_id',\n",
    "                'pop_advertiser_id_conf',\n",
    "                'pop_advertiser_id_conf_multipl',\n",
    "                'pop_campain_id',\n",
    "                'pop_campain_id_conf',\n",
    "                'pop_campain_id_conf_multipl',\n",
    "                'pop_doc_event_doc_ad',\n",
    "                'pop_doc_event_doc_ad_conf',\n",
    "                'pop_doc_event_doc_ad_conf_multipl',\n",
    "                'pop_source_id',  \n",
    "                'pop_source_id_conf',\n",
    "                'pop_source_id_conf_multipl',\n",
    "                'pop_source_id_country',\n",
    "                'pop_source_id_country_conf',\n",
    "                'pop_source_id_country_conf_multipl',\n",
    "                'pop_entity_id',    \n",
    "                'pop_entity_id_conf',\n",
    "                'pop_entity_id_conf_multipl',\n",
    "                'pop_entity_id_country',\n",
    "                'pop_entity_id_country_conf',\n",
    "                'pop_entity_id_country_conf_multipl',\n",
    "                'pop_topic_id', \n",
    "                'pop_topic_id_conf',\n",
    "                'pop_topic_id_conf_multipl',\n",
    "                'pop_topic_id_country',\n",
    "                'pop_topic_id_country_conf',\n",
    "                'pop_topic_id_country_conf_multipl',\n",
    "                'pop_category_id', \n",
    "                'pop_category_id_conf',\n",
    "                'pop_category_id_conf_multipl',\n",
    "                'pop_category_id_country',\n",
    "                'pop_category_id_country_conf',\n",
    "                'pop_category_id_country_conf_multipl',\n",
    "                'user_doc_ad_sim_categories',    \n",
    "                'user_doc_ad_sim_categories_conf',\n",
    "                'user_doc_ad_sim_categories_conf_multipl',\n",
    "                'user_doc_ad_sim_topics',    \n",
    "                'user_doc_ad_sim_topics_conf',\n",
    "                'user_doc_ad_sim_topics_conf_multipl',\n",
    "                'user_doc_ad_sim_entities',                    \n",
    "                'user_doc_ad_sim_entities_conf',\n",
    "                'user_doc_ad_sim_entities_conf_multipl',\n",
    "                'doc_event_doc_ad_sim_categories',    \n",
    "                'doc_event_doc_ad_sim_categories_conf',\n",
    "                'doc_event_doc_ad_sim_categories_conf_multipl',\n",
    "                'doc_event_doc_ad_sim_topics',    \n",
    "                'doc_event_doc_ad_sim_topics_conf',\n",
    "                'doc_event_doc_ad_sim_topics_conf_multipl',\n",
    "                'doc_event_doc_ad_sim_entities',                    \n",
    "                'doc_event_doc_ad_sim_entities_conf',\n",
    "                'doc_event_doc_ad_sim_entities_conf_multipl'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAFFIC_SOURCE_FV='traffic_source'\n",
    "EVENT_HOUR_FV='event_hour'\n",
    "EVENT_COUNTRY_FV = 'event_country'\n",
    "EVENT_COUNTRY_STATE_FV = 'event_country_state'\n",
    "EVENT_GEO_LOCATION_FV = 'event_geo_location'\n",
    "EVENT_PLATFORM_FV = 'event_platform'\n",
    "AD_ADVERTISER_FV = 'ad_advertiser'\n",
    "DOC_AD_SOURCE_ID_FV='doc_ad_source_id'\n",
    "DOC_AD_PUBLISHER_ID_FV='doc_ad_publisher_id'\n",
    "DOC_EVENT_SOURCE_ID_FV='doc_event_source_id'\n",
    "DOC_EVENT_PUBLISHER_ID_FV='doc_event_publisher_id'\n",
    "DOC_AD_CATEGORY_ID_FV='doc_ad_category_id'\n",
    "DOC_AD_TOPIC_ID_FV='doc_ad_topic_id'\n",
    "DOC_AD_ENTITY_ID_FV='doc_ad_entity_id'\n",
    "DOC_EVENT_CATEGORY_ID_FV='doc_event_category_id'\n",
    "DOC_EVENT_TOPIC_ID_FV='doc_event_topic_id'\n",
    "DOC_EVENT_ENTITY_ID_FV='doc_event_entity_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_feature_names_integral = ['ad_advertiser',\n",
    " 'doc_ad_category_id_1',\n",
    " 'doc_ad_category_id_2',\n",
    " 'doc_ad_category_id_3',\n",
    " 'doc_ad_topic_id_1',\n",
    " 'doc_ad_topic_id_2',\n",
    " 'doc_ad_topic_id_3',\n",
    " 'doc_ad_entity_id_1', \n",
    " 'doc_ad_entity_id_2', \n",
    " 'doc_ad_entity_id_3', \n",
    " 'doc_ad_entity_id_4', \n",
    " 'doc_ad_entity_id_5', \n",
    " 'doc_ad_entity_id_6', \n",
    " 'doc_ad_publisher_id',\n",
    " 'doc_ad_source_id', \n",
    " 'doc_event_category_id_1',\n",
    " 'doc_event_category_id_2',\n",
    " 'doc_event_category_id_3',\n",
    " 'doc_event_topic_id_1',\n",
    " 'doc_event_topic_id_2',\n",
    " 'doc_event_topic_id_3',\n",
    " 'doc_event_entity_id_1',\n",
    " 'doc_event_entity_id_2',\n",
    " 'doc_event_entity_id_3',\n",
    " 'doc_event_entity_id_4',\n",
    " 'doc_event_entity_id_5',\n",
    " 'doc_event_entity_id_6',\n",
    " 'doc_event_publisher_id',\n",
    " 'doc_event_source_id', \n",
    " 'event_country',\n",
    " 'event_country_state',\n",
    " 'event_geo_location',\n",
    " 'event_hour',\n",
    " 'event_platform',\n",
    " 'traffic_source']\n",
    "\n",
    "\n",
    "feature_vector_labels_integral = bool_feature_names + int_feature_names + float_feature_names + \\\n",
    "                                 category_feature_names_integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_vector_labels_integral_dict = dict([(key, idx) for idx, key in enumerate(feature_vector_labels_integral)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('feature_vector_labels_integral.txt', 'w') as output:\n",
    "    output.writelines('\\n'.join(feature_vector_labels_integral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_feature_vector_cat_value(field_name, field_value, feature_vector):\n",
    "    if not is_null(field_value) and str(field_value) != '-1':\n",
    "        feature_name = get_ohe_feature_name(field_name, field_value)\n",
    "        if feature_name in feature_vector_labels_dict:\n",
    "            feature_idx = feature_vector_labels_dict[feature_name]\n",
    "        else:\n",
    "            #Unpopular category value\n",
    "            feature_idx = feature_vector_labels_dict[get_ohe_feature_name(field_name, LESS_SPECIAL_CAT_VALUE)]\n",
    "            \n",
    "        feature_vector[feature_idx] = float(1)\n",
    "        \n",
    "def set_feature_vector_cat_values(field_name, field_values, feature_vector):\n",
    "    for field_value in field_values:\n",
    "        set_feature_vector_cat_value(field_name, field_value, feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ad_feature_vector(user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                            event_country, event_country_state,\n",
    "                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                            geo_location_event, \n",
    "                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                            campaign_id, document_id_event,\n",
    "                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                            doc_event_entity_ids, doc_event_ent_confidence_levels):\n",
    "             \n",
    "    try:\n",
    "\n",
    "        feature_vector = {}\n",
    "        \n",
    "        if user_views_count != None:\n",
    "            feature_vector[feature_vector_labels_dict['user_views']] = float(user_views_count)\n",
    "         \n",
    "        if user_doc_ids_viewed != None:\n",
    "            feature_vector[feature_vector_labels_dict['user_has_already_viewed_doc']] = float(document_id in user_doc_ids_viewed)               \n",
    "          \n",
    "        if ad_id in ad_id_popularity_broad.value:            \n",
    "            feature_vector[feature_vector_labels_dict['ad_views']] = float(ad_id_popularity_broad.value[ad_id][1])\n",
    "        \n",
    "        if document_id in document_id_popularity_broad.value:\n",
    "            feature_vector[feature_vector_labels_dict['doc_views']] = float(document_id_popularity_broad.value[document_id][1])            \n",
    "            \n",
    "        if timestamp_event > -1:\n",
    "            dt_timestamp_event = convert_odd_timestamp(timestamp_event)\n",
    "            if doc_ad_publish_time != None:\n",
    "                delta_days = (dt_timestamp_event - doc_ad_publish_time).days\n",
    "                if delta_days >= 0 and delta_days <= 365*10: #10 years\n",
    "                    feature_vector[feature_vector_labels_dict['doc_ad_days_since_published']] = float(delta_days)\n",
    "                        \n",
    "            if doc_event_publish_time != None:\n",
    "                delta_days = (dt_timestamp_event - doc_event_publish_time).days\n",
    "                if delta_days >= 0 and delta_days <= 365*10: #10 years\n",
    "                    feature_vector[feature_vector_labels_dict['doc_event_days_since_published']] = float(delta_days)\n",
    "                    \n",
    "            \n",
    "            #Local period of the day (hours)\n",
    "            dt_local_timestamp_event = get_local_datetime(dt_timestamp_event, event_country, event_country_state)    \n",
    "            local_hour_bin = get_hour_bin(dt_local_timestamp_event.hour)            \n",
    "            feature_vector[feature_vector_labels_dict['doc_event_hour']] = float(local_hour_bin) #Hour for Decision Trees\n",
    "            set_feature_vector_cat_value(EVENT_HOUR_FV, local_hour_bin, feature_vector) #Period of day for FFM\n",
    "            \n",
    "            #Weekend\n",
    "            weekend = int(is_weekend(dt_local_timestamp_event))\n",
    "            feature_vector[feature_vector_labels_dict['event_weekend']] = float(weekend)                                                      \n",
    "        \n",
    "        conf_field_suffix = '_conf'\n",
    "        conf_multiplied_field_suffix = '_conf_multipl'\n",
    "        \n",
    "        #Setting Popularity fields\n",
    "        pop_scores = get_popularity_score(event_country, ad_id, document_id, source_id, \n",
    "                                publisher_id, advertiser_id, campaign_id, document_id_event,\n",
    "                                doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                output_detailed_list=True)\n",
    "        \n",
    "                                \n",
    "\n",
    "        for score in pop_scores:\n",
    "            feature_vector[feature_vector_labels_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "\n",
    "        #Setting User-Doc_ad CB Similarity fields\n",
    "        user_doc_ad_cb_sim_scores = get_user_cb_interest_score(user_views_count, user_categories, user_topics, user_entities, \n",
    "                                timestamp_event, \n",
    "                                 doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                 doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                 doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                output_detailed_list=True)\n",
    "\n",
    "        for score in user_doc_ad_cb_sim_scores:\n",
    "            feature_vector[feature_vector_labels_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "            \n",
    "        #Setting Doc_event-doc_ad CB Similarity fields\n",
    "        doc_event_doc_ad_cb_sim_scores = get_doc_event_doc_ad_cb_similarity_score(\n",
    "                                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                                            doc_event_entity_ids, doc_event_ent_confidence_levels,\n",
    "                                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                        output_detailed_list=True)\n",
    "        \n",
    "        for score in doc_event_doc_ad_cb_sim_scores:\n",
    "            feature_vector[feature_vector_labels_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "            \n",
    "        set_feature_vector_cat_value(TRAFFIC_SOURCE_FV, traffic_source_pv, feature_vector)\n",
    "        set_feature_vector_cat_value(EVENT_COUNTRY_FV, event_country, feature_vector)\n",
    "        set_feature_vector_cat_value(EVENT_COUNTRY_STATE_FV, event_country_state, feature_vector)         \n",
    "        set_feature_vector_cat_value(EVENT_GEO_LOCATION_FV, geo_location_event, feature_vector)\n",
    "        set_feature_vector_cat_value(EVENT_PLATFORM_FV, platform_event, feature_vector)\n",
    "        set_feature_vector_cat_value(AD_ADVERTISER_FV, advertiser_id, feature_vector)\n",
    "        set_feature_vector_cat_value(DOC_AD_SOURCE_ID_FV, source_id, feature_vector)\n",
    "        set_feature_vector_cat_value(DOC_AD_PUBLISHER_ID_FV, publisher_id, feature_vector)\n",
    "        set_feature_vector_cat_value(DOC_EVENT_SOURCE_ID_FV, doc_event_source_id, feature_vector)\n",
    "        set_feature_vector_cat_value(DOC_EVENT_PUBLISHER_ID_FV, doc_event_publisher_id, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_AD_CATEGORY_ID_FV, doc_ad_category_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_AD_TOPIC_ID_FV, doc_ad_topic_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_AD_ENTITY_ID_FV, doc_ad_entity_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_EVENT_CATEGORY_ID_FV, doc_event_category_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_EVENT_TOPIC_ID_FV, doc_event_topic_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_EVENT_ENTITY_ID_FV, doc_event_entity_ids, feature_vector)\n",
    "        \n",
    "        #Creating dummy column as the last column because xgboost have a problem if the last column is undefined for all rows, \n",
    "        #saying that dimentions of data and feature_names do not match\n",
    "        #feature_vector[feature_vector_labels_dict[DUMMY_FEATURE_COLUMN]] = float(0)\n",
    "            \n",
    "        #Ensuring that all elements are floats for compatibility with UDF output (ArrayType(FloatType()))\n",
    "        #feature_vector = list([float(x) for x in feature_vector])\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(\"[get_ad_feature_vector] ERROR PROCESSING FEATURE VECTOR! Params: {}\" \\\n",
    "                        .format([user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                                 event_country, event_country_state,\n",
    "                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                            geo_location_event, \n",
    "                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                            campaign_id, document_id_event,\n",
    "                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                            doc_event_entity_ids, doc_event_ent_confidence_levels]),\n",
    "                        e)\n",
    "    \n",
    "    return SparseVector(len(feature_vector_labels_dict), feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_ad_feature_vector_udf = F.udf(lambda user_doc_ids_viewed, user_views_count, user_categories, user_topics, \n",
    "                                        user_entities, event_country, event_country_state, ad_id, document_id, source_id, \n",
    "                                        doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                                        geo_location_event, \n",
    "                                        doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                                        traffic_source_pv, advertiser_id, publisher_id,\n",
    "                                        campaign_id, document_id_event,\n",
    "                                        category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                        topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                                        entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "                                        doc_event_category_id_list, doc_event_confidence_level_cat_list,\n",
    "                                        doc_event_topic_id_list, doc_event_confidence_level_top,\n",
    "                                        doc_event_entity_id_list, doc_event_confidence_level_ent: \\\n",
    "                                         get_ad_feature_vector(user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                                                            event_country, event_country_state, \n",
    "                                                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                                                            geo_location_event, \n",
    "                                                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,   \n",
    "                                                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                                                            campaign_id, document_id_event,\n",
    "                                                            category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                                            topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                                                            entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "                                                            doc_event_category_id_list, doc_event_confidence_level_cat_list,\n",
    "                                                            doc_event_topic_id_list, doc_event_confidence_level_top,\n",
    "                                                            doc_event_entity_id_list, doc_event_confidence_level_ent),    \n",
    "                            VectorUDT())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_feature_vector_cat_value_integral(field_name, field_value, feature_vector):\n",
    "    if not is_null(field_value): #and str(field_value) != '-1':\n",
    "        feature_vector[feature_vector_labels_integral_dict[field_name]] = float(field_value)\n",
    "        \n",
    "def set_feature_vector_cat_top_multi_values_integral(field_name, values, confidences, feature_vector, top=5):\n",
    "    top_values = list(filter(lambda z: z != -1, map(lambda y: y[0], sorted(zip(values, confidences), key=lambda x: -x[1]))))[:top]\n",
    "    for idx, field_value in list(enumerate(top_values)):\n",
    "        set_feature_vector_cat_value_integral('{}_{}'.format(field_name, idx+1), field_value, feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ad_feature_vector_integral(user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                            event_country, event_country_state,\n",
    "                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                            geo_location_event, \n",
    "                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                            campaign_id, document_id_event,\n",
    "                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                            doc_event_entity_ids, doc_event_ent_confidence_levels):\n",
    "             \n",
    "    try:\n",
    "\n",
    "        feature_vector = {}\n",
    "        \n",
    "        if user_views_count != None:\n",
    "            feature_vector[feature_vector_labels_integral_dict['user_views']] = float(user_views_count)\n",
    "         \n",
    "        if user_doc_ids_viewed != None:\n",
    "            feature_vector[feature_vector_labels_integral_dict['user_has_already_viewed_doc']] = float(document_id in user_doc_ids_viewed)               \n",
    "          \n",
    "        if ad_id in ad_id_popularity_broad.value:            \n",
    "            feature_vector[feature_vector_labels_integral_dict['ad_views']] = float(ad_id_popularity_broad.value[ad_id][1])\n",
    "        \n",
    "        if document_id in document_id_popularity_broad.value:\n",
    "            feature_vector[feature_vector_labels_integral_dict['doc_views']] = float(document_id_popularity_broad.value[document_id][1])            \n",
    "            \n",
    "        if timestamp_event > -1:\n",
    "            dt_timestamp_event = convert_odd_timestamp(timestamp_event)\n",
    "            if doc_ad_publish_time != None:\n",
    "                delta_days = (dt_timestamp_event - doc_ad_publish_time).days\n",
    "                if delta_days >= 0 and delta_days <= 365*10: #10 years\n",
    "                    feature_vector[feature_vector_labels_integral_dict['doc_ad_days_since_published']] = float(delta_days)\n",
    "                        \n",
    "            if doc_event_publish_time != None:\n",
    "                delta_days = (dt_timestamp_event - doc_event_publish_time).days\n",
    "                if delta_days >= 0 and delta_days <= 365*10: #10 years\n",
    "                    feature_vector[feature_vector_labels_integral_dict['doc_event_days_since_published']] = float(delta_days)\n",
    "                    \n",
    "            \n",
    "            #Local period of the day (hours)\n",
    "            dt_local_timestamp_event = get_local_datetime(dt_timestamp_event, event_country, event_country_state)    \n",
    "            local_hour_bin = get_hour_bin(dt_local_timestamp_event.hour)            \n",
    "            feature_vector[feature_vector_labels_integral_dict['doc_event_hour']] = float(local_hour_bin) #Hour for Decision Trees\n",
    "            set_feature_vector_cat_value_integral(EVENT_HOUR_FV, local_hour_bin, feature_vector) #Period of day for FFM\n",
    "            \n",
    "            #Weekend\n",
    "            weekend = int(is_weekend(dt_local_timestamp_event))\n",
    "            feature_vector[feature_vector_labels_integral_dict['event_weekend']] = float(weekend)               \n",
    "                                        \n",
    "        \n",
    "        conf_field_suffix = '_conf'\n",
    "        conf_multiplied_field_suffix = '_conf_multipl'\n",
    "        \n",
    "        #Setting Popularity fields\n",
    "        pop_scores = get_popularity_score(event_country, ad_id, document_id, source_id, \n",
    "                                publisher_id, advertiser_id, campaign_id, document_id_event,\n",
    "                                doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                output_detailed_list=True)\n",
    "        \n",
    "                                \n",
    "\n",
    "        for score in pop_scores:\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "\n",
    "        #Setting User-Doc_ad CB Similarity fields\n",
    "        user_doc_ad_cb_sim_scores = get_user_cb_interest_score(user_views_count, user_categories, user_topics, user_entities, \n",
    "                                timestamp_event, \n",
    "                                 doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                 doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                 doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                output_detailed_list=True)\n",
    "\n",
    "        for score in user_doc_ad_cb_sim_scores:\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "            \n",
    "        #Setting Doc_event-doc_ad CB Similarity fields\n",
    "        doc_event_doc_ad_cb_sim_scores = get_doc_event_doc_ad_cb_similarity_score(\n",
    "                                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                                            doc_event_entity_ids, doc_event_ent_confidence_levels,\n",
    "                                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                        output_detailed_list=True)\n",
    "        \n",
    "        for score in doc_event_doc_ad_cb_sim_scores:\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "        \n",
    "        #Process code for event_country\n",
    "        if event_country in event_country_values_counts:\n",
    "            event_country_code = event_country_values_counts[event_country]\n",
    "        else:\n",
    "            event_country_code = event_country_values_counts[LESS_SPECIAL_CAT_VALUE]                        \n",
    "        set_feature_vector_cat_value_integral(EVENT_COUNTRY_FV, event_country_code, feature_vector)\n",
    "        \n",
    "        #Process code for event_country_state\n",
    "        if event_country_state in event_country_state_values_counts:\n",
    "            event_country_state_code = event_country_state_values_counts[event_country_state]\n",
    "        else:\n",
    "            event_country_state_code = event_country_state_values_counts[LESS_SPECIAL_CAT_VALUE]         \n",
    "        set_feature_vector_cat_value_integral(EVENT_COUNTRY_STATE_FV, event_country_state_code, feature_vector)\n",
    "                \n",
    "        #Process code for geo_location_event\n",
    "        if geo_location_event in event_geo_location_values_counts:\n",
    "            geo_location_event_code = event_geo_location_values_counts[geo_location_event]\n",
    "        else:\n",
    "            geo_location_event_code = event_geo_location_values_counts[LESS_SPECIAL_CAT_VALUE]\n",
    "        set_feature_vector_cat_value_integral(EVENT_GEO_LOCATION_FV, geo_location_event_code, feature_vector)   \n",
    "         \n",
    "        set_feature_vector_cat_value_integral(TRAFFIC_SOURCE_FV, traffic_source_pv, feature_vector)        \n",
    "        set_feature_vector_cat_value_integral(EVENT_PLATFORM_FV, platform_event, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(AD_ADVERTISER_FV, advertiser_id, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(DOC_AD_SOURCE_ID_FV, source_id, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(DOC_AD_PUBLISHER_ID_FV, publisher_id, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(DOC_EVENT_SOURCE_ID_FV, doc_event_source_id, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(DOC_EVENT_PUBLISHER_ID_FV, doc_event_publisher_id, feature_vector)\n",
    "                \n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_AD_CATEGORY_ID_FV, doc_ad_category_ids, doc_ad_cat_confidence_levels, feature_vector, top=3)\n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_AD_TOPIC_ID_FV, doc_ad_topic_ids, doc_ad_top_confidence_levels, feature_vector, top=3)\n",
    "        \n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_EVENT_CATEGORY_ID_FV, doc_event_category_ids, doc_event_cat_confidence_levels, feature_vector, top=3)\n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_EVENT_TOPIC_ID_FV, doc_event_topic_ids, doc_event_top_confidence_levels, feature_vector, top=3)                           \n",
    "        \n",
    "        #Process codes for doc_ad_entity_ids\n",
    "        doc_ad_entity_ids_codes = [doc_entity_id_values_counts[x] if x in doc_entity_id_values_counts \n",
    "                                   else doc_entity_id_values_counts[LESS_SPECIAL_CAT_VALUE] \n",
    "                                   for x in doc_ad_entity_ids]\n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_AD_ENTITY_ID_FV, doc_ad_entity_ids_codes, doc_ad_ent_confidence_levels, feature_vector, top=6)\n",
    "        \n",
    "        \n",
    "        #Process codes for doc_event_entity_ids\n",
    "        doc_event_entity_ids_codes = [doc_entity_id_values_counts[x] if x in doc_entity_id_values_counts \n",
    "                                   else doc_entity_id_values_counts[LESS_SPECIAL_CAT_VALUE] \n",
    "                                   for x in doc_event_entity_ids]\n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_EVENT_ENTITY_ID_FV, doc_event_entity_ids_codes, doc_event_ent_confidence_levels, feature_vector, top=6)\n",
    "        \n",
    "        #Creating dummy column as the last column because xgboost have a problem if the last column is undefined for all rows, \n",
    "        #saying that dimentions of data and feature_names do not match\n",
    "        #feature_vector[feature_vector_labels_dict[DUMMY_FEATURE_COLUMN]] = float(0)\n",
    "            \n",
    "        #Ensuring that all elements are floats for compatibility with UDF output (ArrayType(FloatType()))\n",
    "        #feature_vector = list([float(x) for x in feature_vector])\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(\"[get_ad_feature_vector_integral] ERROR PROCESSING FEATURE VECTOR! Params: {}\" \\\n",
    "                        .format([user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                                 event_country, event_country_state,\n",
    "                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                            geo_location_event, \n",
    "                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                            campaign_id, document_id_event,\n",
    "                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                            doc_event_entity_ids, doc_event_ent_confidence_levels]),\n",
    "                        e)\n",
    "    \n",
    "    return SparseVector(len(feature_vector_labels_integral_dict), feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_ad_feature_vector_integral_udf = F.udf(lambda user_doc_ids_viewed, user_views_count, user_categories, user_topics, \n",
    "                                        user_entities, event_country, event_country_state, ad_id, document_id, source_id, \n",
    "                                        doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                                        geo_location_event, \n",
    "                                        doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                                        traffic_source_pv, advertiser_id, publisher_id,\n",
    "                                        campaign_id, document_id_event,\n",
    "                                        category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                        topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                                        entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "                                        doc_event_category_id_list, doc_event_confidence_level_cat_list,\n",
    "                                        doc_event_topic_id_list, doc_event_confidence_level_top,\n",
    "                                        doc_event_entity_id_list, doc_event_confidence_level_ent: \\\n",
    "                                         get_ad_feature_vector_integral(user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                                                            event_country, event_country_state, \n",
    "                                                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                                                            geo_location_event, \n",
    "                                                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,   \n",
    "                                                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                                                            campaign_id, document_id_event,\n",
    "                                                            category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                                            topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                                                            entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "                                                            doc_event_category_id_list, doc_event_confidence_level_cat_list,\n",
    "                                                            doc_event_topic_id_list, doc_event_confidence_level_top,\n",
    "                                                            doc_event_entity_id_list, doc_event_confidence_level_ent),    \n",
    "                            VectorUDT())\n",
    "                             #StructField(\"features\", VectorUDT()))\n",
    "                             #MapType(IntegerType(), FloatType())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Train set feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_enriched_df = train_set_df \\\n",
    "                             .join(documents_categories_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_categories_grouped.document_id_cat\"), how='left') \\\n",
    "                             .join(documents_topics_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_topics_grouped.document_id_top\"), how='left') \\\n",
    "                             .join(documents_entities_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_entities_grouped.document_id_ent\"), how='left') \\\n",
    "                             .join(documents_categories_grouped_df \\\n",
    "                                       .withColumnRenamed('category_id_list', 'doc_event_category_id_list')\n",
    "                                       .withColumnRenamed('confidence_level_cat_list', 'doc_event_confidence_level_cat_list') \\\n",
    "                                       .alias('documents_event_categories_grouped'), \n",
    "                                   on=F.col(\"document_id_event\") == F.col(\"documents_event_categories_grouped.document_id_cat\"), \n",
    "                                   how='left') \\\n",
    "                             .join(documents_topics_grouped_df \\\n",
    "                                       .withColumnRenamed('topic_id_list', 'doc_event_topic_id_list')\n",
    "                                       .withColumnRenamed('confidence_level_top_list', 'doc_event_confidence_level_top_list') \\\n",
    "                                       .alias('documents_event_topics_grouped'), \n",
    "                                   on=F.col(\"document_id_event\") == F.col(\"documents_event_topics_grouped.document_id_top\"), \n",
    "                                   how='left') \\\n",
    "                             .join(documents_entities_grouped_df \\\n",
    "                                       .withColumnRenamed('entity_id_list', 'doc_event_entity_id_list')\n",
    "                                       .withColumnRenamed('confidence_level_ent_list', 'doc_event_confidence_level_ent_list') \\\n",
    "                                       .alias('documents_event_entities_grouped'), \n",
    "                                   on=F.col(\"document_id_event\") == F.col(\"documents_event_entities_grouped.document_id_ent\"), \n",
    "                                   how='left') \\\n",
    "                            .select('display_id','uuid_event','event_country','event_country_state','platform_event',\n",
    "                                    'source_id_doc_event', 'publisher_doc_event','publish_time_doc_event',\n",
    "                                            'publish_time', 'ad_id','document_id_promo','clicked',   \n",
    "                                           'geo_location_event', 'advertiser_id', 'publisher_id',\n",
    "                                            'campaign_id', 'document_id_event',\n",
    "                                            'traffic_source_pv',                                          \n",
    "                                        int_list_null_to_empty_list_udf('doc_event_category_id_list').alias('doc_event_category_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_cat_list').alias('doc_event_confidence_level_cat_list'),\n",
    "                                        int_list_null_to_empty_list_udf('doc_event_topic_id_list').alias('doc_event_topic_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_top_list').alias('doc_event_confidence_level_top_list'),\n",
    "                                        str_list_null_to_empty_list_udf('doc_event_entity_id_list').alias('doc_event_entity_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_ent_list').alias('doc_event_confidence_level_ent_list'),\n",
    "                                       int_null_to_minus_one_udf('source_id').alias('source_id'),                                      \n",
    "                                       int_null_to_minus_one_udf('timestamp_event').alias('timestamp_event'),\n",
    "                                       int_list_null_to_empty_list_udf('category_id_list').alias('category_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_cat_list').alias('confidence_level_cat_list'), \n",
    "                                       int_list_null_to_empty_list_udf('topic_id_list').alias('topic_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_top_list').alias('confidence_level_top_list'), \n",
    "                                       str_list_null_to_empty_list_udf('entity_id_list').alias('entity_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_ent_list').alias('confidence_level_ent_list')                                                       \n",
    "                                      ) \\\n",
    "                            .join(user_profiles_df, on=[F.col(\"user_profiles.uuid\") == F.col(\"uuid_event\")], how='left') \\\n",
    "                            .withColumnRenamed('categories', 'user_categories') \\\n",
    "                            .withColumnRenamed('topics', 'user_topics') \\\n",
    "                            .withColumnRenamed('entities', 'user_entities') \\\n",
    "                            .withColumnRenamed('doc_ids', 'user_doc_ids_viewed') \\\n",
    "                            .withColumnRenamed('views', 'user_views_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_feature_vectors_df = train_set_enriched_df \\\n",
    "                                .withColumn('feature_vector', \n",
    "                                            #get_ad_feature_vector_udf(\n",
    "                                            get_ad_feature_vector_integral_udf(\n",
    "                                                                'user_doc_ids_viewed',\n",
    "                                                                'user_views_count',\n",
    "                                                                'user_categories', \n",
    "                                                                'user_topics', \n",
    "                                                                'user_entities', \n",
    "                                                                'event_country', \n",
    "                                                                'event_country_state',\n",
    "                                                                'ad_id', \n",
    "                                                                'document_id_promo', \n",
    "                                                                'source_id', \n",
    "                                                                'publish_time', \n",
    "                                                                'timestamp_event', \n",
    "                                                                'platform_event',\n",
    "                                                                'geo_location_event', \n",
    "                                                                'source_id_doc_event', \n",
    "                                                                'publisher_doc_event',\n",
    "                                                                'publish_time_doc_event',\n",
    "                                                                'traffic_source_pv',\n",
    "                                                                'advertiser_id', \n",
    "                                                                'publisher_id',\n",
    "                                                                'campaign_id',\n",
    "                                                                'document_id_event',\n",
    "                                                                'category_id_list', \n",
    "                                                                'confidence_level_cat_list', \n",
    "                                                                'topic_id_list', \n",
    "                                                                'confidence_level_top_list',\n",
    "                                                                'entity_id_list', \n",
    "                                                                'confidence_level_ent_list',\n",
    "                                                                'doc_event_category_id_list',\n",
    "                                                                'doc_event_confidence_level_cat_list',\n",
    "                                                                'doc_event_topic_id_list',\n",
    "                                                                'doc_event_confidence_level_top_list',\n",
    "                                                                'doc_event_entity_id_list',\n",
    "                                                                'doc_event_confidence_level_ent_list')) \\\n",
    "                            .select(F.col('uuid_event').alias('uuid'),\n",
    "                                    'display_id',\n",
    "                                    'ad_id',\n",
    "                                    'document_id_event',\n",
    "                                    F.col('document_id_promo').alias('document_id'),\n",
    "                                    F.col('clicked').alias('label'),\n",
    "                                    'feature_vector') #\\\n",
    "                            #.orderBy('display_id','ad_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    train_feature_vector_gcs_folder_name = 'train_feature_vectors_integral_eval'\n",
    "else:\n",
    "    train_feature_vector_gcs_folder_name = 'train_feature_vectors_integral'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 984 ms, sys: 180 ms, total: 1.16 s\n",
      "Wall time: 1h 45min 8s\n"
     ]
    }
   ],
   "source": [
    "%time train_set_feature_vectors_df.write.parquet(OUTPUT_BUCKET_FOLDER+train_feature_vector_gcs_folder_name, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting integral feature vectors to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(uuid='100013af048bbf', display_id=16757900, ad_id=147242, document_id_event=38915, document_id=1108162, label=0, feature_vector=SparseVector(103, {0: 1.0, 3: 8312.0, 4: 106055.0, 5: 1536.0, 6: 6.0, 8: 0.0852, 9: 0.5434, 10: 0.0463, 11: 0.1296, 12: 0.4793, 13: 0.0621, 17: 0.1358, 18: 0.4796, 19: 0.0651, 20: 0.0834, 21: 0.4729, 22: 0.0394, 23: 0.1844, 24: 0.2046, 25: 0.0377, 26: 0.1327, 27: 0.4602, 28: 0.0611, 29: 0.1327, 30: 0.4602, 31: 0.0611, 32: 0.1327, 33: 0.0474, 34: 0.0063, 35: 0.1327, 36: 0.0474, 37: 0.0063, 38: 0.16, 39: 0.0116, 40: 0.0018, 41: 0.164, 42: 0.0127, 43: 0.0021, 44: 0.1599, 45: 0.0788, 46: 0.0126, 47: 0.1569, 48: 0.0759, 49: 0.0119, 59: 0.0, 60: 0.0004, 61: 0.0, 62: 0.0, 63: 0.0001, 64: 0.0, 68: 2241.0, 69: 2004.0, 70: 1513.0, 72: 26.0, 82: 9676.0, 83: 1205.0, 84: 2003.0, 86: 200.0, 87: 67.0, 88: 72.0, 95: 240.0, 96: 9462.0, 97: 18595452.0, 98: 2395278.0, 99: 695787.0, 100: 6.0, 101: 3.0, 102: 2.0})),\n",
       " Row(uuid='100013af048bbf', display_id=16757900, ad_id=7033, document_id_event=38915, document_id=393333, label=0, feature_vector=SparseVector(103, {0: 1.0, 3: 8360.0, 4: 22043.0, 5: 1536.0, 6: 6.0, 7: 640.0, 8: 0.1205, 9: 0.5437, 10: 0.0655, 11: 0.1099, 12: 0.536, 13: 0.0589, 17: 0.1122, 18: 0.5053, 19: 0.0567, 20: 0.1122, 21: 0.5053, 22: 0.0567, 23: 0.1707, 24: 0.3714, 25: 0.0634, 26: 0.1122, 27: 0.5053, 28: 0.0567, 29: 0.1122, 30: 0.5053, 31: 0.0567, 32: 0.1635, 33: 0.0255, 34: 0.0042, 35: 0.1643, 36: 0.0256, 37: 0.0042, 38: 0.16, 39: 0.0096, 40: 0.0015, 41: 0.164, 42: 0.0106, 43: 0.0017, 44: 0.1231, 45: 0.2309, 46: 0.0284, 47: 0.1234, 48: 0.2464, 49: 0.0304, 59: 0.0, 60: 0.0004, 61: 0.0, 62: 0.0, 63: 0.0001, 64: 0.0, 68: 386.0, 69: 1206.0, 70: 1208.0, 72: 26.0, 75: 448.0, 82: 4773.0, 83: 1205.0, 84: 2003.0, 86: 200.0, 87: 67.0, 88: 72.0, 95: 240.0, 96: 9462.0, 97: 18595452.0, 98: 2395278.0, 99: 695787.0, 100: 6.0, 101: 3.0, 102: 2.0})),\n",
       " Row(uuid='100013af048bbf', display_id=16757900, ad_id=156270, document_id_event=38915, document_id=1388416, label=1, feature_vector=SparseVector(103, {0: 1.0, 3: 23220.0, 4: 24827.0, 5: 1536.0, 6: 6.0, 7: 81.0, 8: 0.2974, 9: 0.6052, 10: 0.18, 11: 0.2933, 12: 0.5675, 13: 0.1665, 17: 0.2069, 18: 0.4617, 19: 0.0955, 20: 0.2933, 21: 0.5675, 22: 0.1665, 23: 0.5655, 24: 0.3416, 25: 0.1932, 26: 0.2069, 27: 0.4617, 28: 0.0955, 29: 0.2522, 30: 0.4203, 31: 0.106, 38: 0.1995, 39: 0.0014, 40: 0.0003, 41: 0.2064, 42: 0.0014, 43: 0.0003, 44: 0.2277, 45: 0.0782, 46: 0.0178, 47: 0.2319, 48: 0.0746, 49: 0.0173, 59: 0.0, 60: 0.0004, 61: 0.0, 62: 0.0, 63: 0.0002, 64: 0.0, 68: 2623.0, 69: 1403.0, 70: 1405.0, 72: 43.0, 73: 258.0, 74: 136.0, 82: 10770.0, 83: 1205.0, 84: 2003.0, 86: 200.0, 87: 67.0, 88: 72.0, 95: 240.0, 96: 9462.0, 97: 18595452.0, 98: 2395278.0, 99: 695787.0, 100: 6.0, 101: 3.0, 102: 2.0}))]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature_vectors_exported_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER+train_feature_vector_gcs_folder_name)\n",
    "train_feature_vectors_exported_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    train_feature_vector_integral_csv_folder_name = 'train_feature_vectors_integral_eval.csv'\n",
    "else:\n",
    "    train_feature_vector_integral_csv_folder_name = 'train_feature_vectors_integral.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "integral_headers = ['label', 'display_id', 'ad_id', 'doc_id', 'doc_event_id', 'is_leak'] + feature_vector_labels_integral\n",
    "    \n",
    "with open(train_feature_vector_integral_csv_folder_name+\".header\", 'w') as output:\n",
    "    output.writelines('\\n'.join(integral_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparse_vector_to_csv_with_nulls_row(additional_column_values, vec, num_columns):    \n",
    "    return ','.join([str(value) for value in additional_column_values] + \n",
    "                     list([ '{:.5}'.format(vec[x]) if x in vec.indices else '' for x in range(vec.size) ])[:num_columns]) \\\n",
    "            .replace('.0,',',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feature_vectors_integral_csv_rdd = train_feature_vectors_exported_df.select(\n",
    "     'label', 'display_id', 'ad_id', 'document_id', 'document_id_event', 'feature_vector').withColumn('is_leak', F.lit(-1)) \\\n",
    "     .rdd.map(lambda x: sparse_vector_to_csv_with_nulls_row([x['label'], x['display_id'], x['ad_id'], x['document_id'], x['document_id_event'], x['is_leak']], \n",
    "                                                  x['feature_vector'], len(integral_headers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time train_feature_vectors_integral_csv_rdd.saveAsTextFile(OUTPUT_BUCKET_FOLDER+train_feature_vector_integral_csv_folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Validation/Test set feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_leak(max_timestamp_pv_leak, timestamp_event):\n",
    "    return max_timestamp_pv_leak >= 0 and max_timestamp_pv_leak >= timestamp_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_leak_udf = F.udf(lambda max_timestamp_pv_leak, timestamp_event: int(is_leak(max_timestamp_pv_leak, timestamp_event)), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    data_df = validation_set_df\n",
    "else:\n",
    "    data_df = test_set_df\n",
    "\n",
    "\n",
    "test_validation_set_enriched_df = data_df.select('display_id','uuid_event','event_country','event_country_state','platform_event',\n",
    "                                            'source_id_doc_event', 'publisher_doc_event','publish_time_doc_event',     \n",
    "                                            'publish_time',\n",
    "                                           'ad_id','document_id_promo','clicked',  \n",
    "                                           'geo_location_event', 'advertiser_id', 'publisher_id',\n",
    "                                           'campaign_id', 'document_id_event',\n",
    "                                           'traffic_source_pv',                                           \n",
    "                                        int_list_null_to_empty_list_udf('doc_event_category_id_list').alias('doc_event_category_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_cat_list').alias('doc_event_confidence_level_cat_list'),\n",
    "                                        int_list_null_to_empty_list_udf('doc_event_topic_id_list').alias('doc_event_topic_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_top_list').alias('doc_event_confidence_level_top_list'),\n",
    "                                        str_list_null_to_empty_list_udf('doc_event_entity_id_list').alias('doc_event_entity_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_ent_list').alias('doc_event_confidence_level_ent_list'),\n",
    "                                       int_null_to_minus_one_udf('source_id').alias('source_id'),                                   \n",
    "                                       int_null_to_minus_one_udf('timestamp_event').alias('timestamp_event'),\n",
    "                                       int_list_null_to_empty_list_udf('category_id_list').alias('category_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_cat_list').alias('confidence_level_cat_list'), \n",
    "                                       int_list_null_to_empty_list_udf('topic_id_list').alias('topic_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_top_list').alias('confidence_level_top_list'), \n",
    "                                       str_list_null_to_empty_list_udf('entity_id_list').alias('entity_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_ent_list').alias('confidence_level_ent_list'),\n",
    "                                       int_null_to_minus_one_udf('max_timestamp_pv').alias('max_timestamp_pv_leak')\n",
    "                                      ) \\\n",
    "                            .join(user_profiles_df, on=[F.col(\"user_profiles.uuid\") == F.col(\"uuid_event\")], how='left') \\\n",
    "                            .withColumnRenamed('categories', 'user_categories') \\\n",
    "                            .withColumnRenamed('topics', 'user_topics') \\\n",
    "                            .withColumnRenamed('entities', 'user_entities') \\\n",
    "                            .withColumnRenamed('doc_ids', 'user_doc_ids_viewed') \\\n",
    "                            .withColumnRenamed('views', 'user_views_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_validation_set_feature_vectors_df = test_validation_set_enriched_df \\\n",
    "                                .withColumn('feature_vector', \n",
    "                                            #get_ad_feature_vector_udf(\n",
    "                                            get_ad_feature_vector_integral_udf(\n",
    "                                                                'user_doc_ids_viewed', \n",
    "                                                                'user_views_count',\n",
    "                                                                'user_categories', \n",
    "                                                                'user_topics', \n",
    "                                                                'user_entities', \n",
    "                                                                'event_country', \n",
    "                                                                'event_country_state',\n",
    "                                                                'ad_id', \n",
    "                                                                'document_id_promo', \n",
    "                                                                'source_id', \n",
    "                                                                'publish_time', \n",
    "                                                                'timestamp_event', \n",
    "                                                                'platform_event',\n",
    "                                                                'geo_location_event', \n",
    "                                                                'source_id_doc_event', \n",
    "                                                                'publisher_doc_event',\n",
    "                                                                'publish_time_doc_event',\n",
    "                                                                'traffic_source_pv',\n",
    "                                                                'advertiser_id', \n",
    "                                                                'publisher_id',\n",
    "                                                                'campaign_id',\n",
    "                                                                'document_id_event',\n",
    "                                                                'category_id_list', \n",
    "                                                                'confidence_level_cat_list', \n",
    "                                                                'topic_id_list', \n",
    "                                                                'confidence_level_top_list',\n",
    "                                                                'entity_id_list', \n",
    "                                                                'confidence_level_ent_list',\n",
    "                                                                'doc_event_category_id_list',\n",
    "                                                                'doc_event_confidence_level_cat_list',\n",
    "                                                                'doc_event_topic_id_list',\n",
    "                                                                'doc_event_confidence_level_top_list',\n",
    "                                                                'doc_event_entity_id_list',\n",
    "                                                                'doc_event_confidence_level_ent_list')) \\\n",
    "                            .select(F.col('uuid').alias('uuid'),                                    \n",
    "                                    'display_id',\n",
    "                                    'ad_id',\n",
    "                                    'document_id_event',\n",
    "                                    F.col('document_id_promo').alias('document_id'),\n",
    "                                    F.col('clicked').alias('label'),\n",
    "                                    is_leak_udf('max_timestamp_pv_leak','timestamp_event').alias('is_leak'),\n",
    "                                    'feature_vector') #\\\n",
    "                            #.orderBy('display_id','ad_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    test_validation_feature_vector_gcs_folder_name = 'validation_feature_vectors_integral'\n",
    "else:\n",
    "    test_validation_feature_vector_gcs_folder_name = 'test_feature_vectors_integral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 640 ms, sys: 184 ms, total: 824 ms\n",
      "Wall time: 1h 7min 15s\n"
     ]
    }
   ],
   "source": [
    "%time test_validation_set_feature_vectors_df.write.parquet(OUTPUT_BUCKET_FOLDER+test_validation_feature_vector_gcs_folder_name, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting integral feature vectors to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(uuid='1003a7cb5ec2bc', display_id=17199427, ad_id=186645, document_id_event=1147007, document_id=1464868, label=-999, is_leak=0, feature_vector=SparseVector(103, {0: 0.0, 1: 0.0, 2: 2.0, 3: 16015.0, 4: 16021.0, 5: 97.0, 6: 4.0, 8: 0.1986, 9: 0.5829, 10: 0.1157, 11: 0.1986, 12: 0.5412, 13: 0.1075, 17: 0.1444, 18: 0.437, 19: 0.0631, 20: 0.1986, 21: 0.5412, 22: 0.1075, 23: 0.1142, 24: 0.3752, 25: 0.0428, 26: 0.1444, 27: 0.437, 28: 0.0631, 29: 0.1195, 30: 0.4005, 31: 0.0479, 38: 0.1666, 39: 0.0086, 40: 0.0014, 41: 0.1758, 42: 0.0051, 43: 0.0009, 44: 0.2105, 45: 0.1395, 46: 0.0294, 47: 0.1886, 48: 0.1943, 49: 0.0366, 50: 0.0, 51: 0.0009, 52: 0.0, 53: 0.0, 54: 0.0001, 55: 0.0, 59: 0.0, 60: 0.0004, 61: 0.0, 62: 0.0, 63: 0.0001, 64: 0.0, 68: 644.0, 69: 1302.0, 70: 1408.0, 72: 2.0, 82: 359.0, 83: 1403.0, 84: 1610.0, 86: 138.0, 87: 163.0, 88: 92.0, 95: 450.0, 96: 7736.0, 97: 1117544.0, 98: 141998.0, 99: 141998.0, 100: 4.0, 101: 2.0, 102: 1.0})),\n",
       " Row(uuid='1003a7cb5ec2bc', display_id=17199427, ad_id=153016, document_id_event=1147007, document_id=1360929, label=-999, is_leak=0, feature_vector=SparseVector(103, {0: 0.0, 1: 0.0, 2: 2.0, 3: 29581.0, 4: 108203.0, 5: 97.0, 6: 4.0, 7: 48.0, 8: 0.6043, 9: 0.6198, 10: 0.3746, 11: 0.5644, 12: 0.5535, 13: 0.3124, 17: 0.5937, 18: 0.4342, 19: 0.2578, 20: 0.5996, 21: 0.6078, 22: 0.3644, 23: 0.4316, 24: 0.3581, 25: 0.1545, 26: 0.5496, 27: 0.4563, 28: 0.2508, 29: 0.5948, 30: 0.5129, 31: 0.3051, 32: 0.4927, 33: 0.0658, 34: 0.0324, 35: 0.5769, 36: 0.0556, 37: 0.032, 38: 0.2175, 39: 0.0276, 40: 0.006, 41: 0.2404, 42: 0.0286, 43: 0.0069, 44: 0.2271, 45: 0.1308, 46: 0.0297, 47: 0.2167, 48: 0.1313, 49: 0.0285, 50: 0.3349, 51: 0.9991, 52: 0.3346, 53: 0.0, 54: 0.0002, 55: 0.0, 56: 0.0, 57: 0.0, 58: 0.0, 59: 0.9605, 60: 0.9996, 61: 0.9601, 62: 0.0, 63: 0.0001, 64: 0.0, 65: 0.0, 66: 0.0, 67: 0.0, 68: 1559.0, 69: 1403.0, 70: 1510.0, 72: 160.0, 73: 227.0, 75: 146.0, 76: 95.0, 77: 334.0, 82: 7698.0, 83: 1403.0, 84: 1610.0, 86: 138.0, 87: 163.0, 88: 92.0, 95: 450.0, 96: 7736.0, 97: 1117544.0, 98: 141998.0, 99: 141998.0, 100: 4.0, 101: 2.0, 102: 1.0})),\n",
       " Row(uuid='1003a7cb5ec2bc', display_id=17199427, ad_id=189845, document_id_event=1147007, document_id=1452059, label=-999, is_leak=0, feature_vector=SparseVector(103, {0: 0.0, 1: 0.0, 2: 2.0, 3: 4608.0, 4: 4608.0, 5: 97.0, 6: 4.0, 7: 41.0, 8: 0.2941, 9: 0.5079, 10: 0.1493, 11: 0.2941, 12: 0.5079, 13: 0.1493, 17: 0.2774, 18: 0.4246, 19: 0.1178, 20: 0.294, 21: 0.4418, 22: 0.1299, 23: 0.1758, 24: 0.3493, 25: 0.0614, 26: 0.2774, 27: 0.4246, 28: 0.1178, 29: 0.2774, 30: 0.4246, 31: 0.1178, 32: 0.2711, 33: 0.0793, 34: 0.0215, 35: 0.2711, 36: 0.0793, 37: 0.0215, 38: 0.2144, 39: 0.0004, 40: 0.0001, 41: 0.178, 42: 0.0003, 43: 0.0001, 44: 0.2144, 45: 0.0658, 46: 0.0141, 47: 0.1889, 48: 0.0651, 49: 0.0123, 50: 0.0, 51: 0.0009, 52: 0.0, 53: 0.0, 54: 0.0002, 55: 0.0, 56: 0.0, 57: 0.0, 58: 0.0, 59: 0.0, 60: 0.0004, 61: 0.0, 62: 0.0, 63: 0.0001, 64: 0.0, 65: 0.0, 66: 0.0, 67: 0.0, 68: 363.0, 69: 1608.0, 70: 1408.0, 72: 213.0, 73: 20.0, 75: 15.0, 82: 2460.0, 83: 1403.0, 84: 1610.0, 86: 138.0, 87: 163.0, 88: 92.0, 95: 450.0, 96: 7736.0, 97: 1117544.0, 98: 141998.0, 99: 141998.0, 100: 4.0, 101: 2.0, 102: 1.0}))]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_validation_feature_vectors_exported_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER+test_validation_feature_vector_gcs_folder_name)\n",
    "test_validation_feature_vectors_exported_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    test_validation_feature_vector_integral_csv_folder_name = 'validation_feature_vectors_integral.csv'\n",
    "else:\n",
    "    test_validation_feature_vector_integral_csv_folder_name = 'test_feature_vectors_integral.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "integral_headers = ['label', 'display_id', 'ad_id', 'doc_id', 'doc_event_id', 'is_leak'] + feature_vector_labels_integral\n",
    "    \n",
    "with open(test_validation_feature_vector_integral_csv_folder_name+\".header\", 'w') as output:\n",
    "    output.writelines('\\n'.join(integral_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_validation_feature_vectors_integral_csv_rdd = test_validation_feature_vectors_exported_df.select(\n",
    "     'label', 'display_id', 'ad_id', 'document_id', 'document_id_event', 'is_leak', 'feature_vector') \\\n",
    "     .rdd.map(lambda x: sparse_vector_to_csv_with_nulls_row([x['label'], x['display_id'], x['ad_id'], x['document_id'], x['document_id_event'], x['is_leak']], \n",
    "                                                  x['feature_vector'], len(integral_headers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time test_validation_feature_vectors_integral_csv_rdd.saveAsTextFile(OUTPUT_BUCKET_FOLDER+test_validation_feature_vector_integral_csv_folder_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
